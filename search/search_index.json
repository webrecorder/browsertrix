{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Browsertrix Docs","text":"<p>Browsertrix is an open source web archiving system created by Webrecorder. Sign up for Browsertrix to start archiving with zero setup, or follow our self-hosting guide to deploy Browsertrix on your own infrastructure.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Docs are organized into the following sections:</p> <ul> <li>User Guide \u2014 Instructions on how to use Browsertrix to create and share web archives.</li> <li>Self-Hosting \u2014 Instructions on how to install, set up, and deploy self-hosted Browsertrix.</li> <li>Development \u2014 Contribute to the open source development of Browsertrix software.</li> <li>API Reference \u2014 Full API reference for interacting with the Browsertrix backend.</li> </ul> <p>If you have feedback on the docs, please feel free to reach out to us.</p>"},{"location":"#support","title":"Support","text":"<p>For help with a specific topic, try our community help forum.</p> <p>Dedicated professional support is available with a custom subscription or support contract. Check out our plans for details.</p>"},{"location":"#bugs","title":"Bugs","text":"<p>For bug reports or feature requests, please open a GitHub issue.</p>"},{"location":"deploy/","title":"Self-Hosting","text":"<p>Already signed up for Browsertrix?</p> <p>This guide is for developers and users who are self-hosting Browsertrix. If you've registered through webrecorder.net, you may be looking for the user guide.</p> <p>Browsertrix is designed to be a cloud-native application running in Kubernetes.</p> <p>However, it is perfectly reasonable to deploy Browsertrix locally using one of the many available local Kubernetes options.</p> <p>The main requirements for Browsertrix are:</p> <ul> <li>A Kubernetes Cluster</li> <li>Helm 3 (package manager for Kubernetes)</li> </ul> <p>We have prepared a Local Deployment Guide which covers several options for testing Browsertrix locally on a single machine, as well as a Production (Self-Hosted and Cloud) Deployment guide to help with setting up Browsertrix in different production scenarios. Information about configuring storage, crawler channels, and other details in local or production deployments is in the Customizing Browsertrix Deployment Guide. Information about configuring proxies to use with Browsertrix can be found in the Configuring Proxies guide.</p> <p>Details on managing org export and import for existing clusters can be found in the Org Import &amp; Export guide.</p>"},{"location":"deploy/customization/","title":"Customizing Browsertrix Deployment","text":"<p>Local and production deployments alike can be customized by modifying the <code>chart/values.yaml</code> Helm chart file or a local override. For more on using local overrides, see the Local Deployment Guide. The remainder of this guide covers some of the customization options available in the Helm chart.</p>"},{"location":"deploy/customization/#default-organization","title":"Default Organization","text":"<p>The <code>default_org</code> setting is used to specify the name for the default organization created in a Browsertrix deployment. A slug will be auto-generated based on this value and can be modified in Org Settings within the application.</p>"},{"location":"deploy/customization/#superuser","title":"Superuser","text":"<p>The <code>superuser</code> setting is used to set the username and password for a deployment's superuser. If <code>password</code> is left blank, the application will auto-generate a secure password for the superuser.</p>"},{"location":"deploy/customization/#crawler-channels","title":"Crawler Channels","text":"<p>The <code>crawler_channels</code> setting is used to specify the Crawler Release Channel option available to users via dropdown menus in workflows and browser profiles. Each crawler channel has an id and a Docker image tag. These channels are modifiable with the restriction that there must always be one channel with the id <code>default</code>. By default this is the only channel available on deployments:</p> <pre><code>crawler_channels:\n  - id: default\n    image: \"docker.io/webrecorder/browsertrix-crawler:latest\"\n    imagePullPolicy: Always # optional\n</code></pre> <p>This can be extended with additional channels. For example, here is what the value would look like adding a new x.y.z release of Browsertrix Crawler with the id <code>testing</code>:</p> <pre><code>crawler_channels:\n  - id: default\n    image: \"docker.io/webrecorder/browsertrix-crawler:latest\"\n  - id: testing\n    image: \"docker.io/webrecorder/browsertrix-crawler:x.y.z\"\n    imagePullPolicy: IfNotPresent\n</code></pre> <p>The <code>imagePullPolicy</code> per channel is optional. If not set, the value set in <code>crawler_pull_policy</code> is used as the default.</p>"},{"location":"deploy/customization/#storage","title":"Storage","text":"<p>The <code>storage</code> setting is used to specify primary and replica storage for a Browsertrix deployment. All configured storage options must be S3-compatible buckets. At minimum, there must be one configured storage option, which includes a <code>is_default_primary: true</code>.</p>"},{"location":"deploy/customization/#using-local-minio-storage","title":"Using Local Minio Storage","text":"<p>Browsertrix includes a built-in Minio storage service, which is enabled by default (<code>minio_local: true</code> is set).</p> <p>The configuration for this is as follows:</p> <pre><code>storages:\n  - name: \"default\"\n    type: \"s3\"\n    access_key: \"ADMIN\"\n    secret_key: \"PASSW0RD\"\n    bucket_name: btrix-data\n\n    endpoint_url: \"http://local-minio.default:9000/\"\n    access_endpoint_url: /data/\n</code></pre> <p>The <code>access_key</code> and <code>secret_key</code> should be changed, otherwise no additional changes are needed, and all local data will be stored in this Minio instance by default.</p> <p>The S3 bucket is accessible via <code>/data/</code> path on the same host Browsertrix is running on, eg. <code>http://localhost:30870/data/</code>.</p>"},{"location":"deploy/customization/#using-external-s3-storage-providers","title":"Using External S3 Storage Providers","text":"<p>Browsertrix can also be used with external S3 storage providers, which can be configured as follows:</p> <pre><code>storages:\n  - name: default\n    type: \"s3\"\n    access_key: \"accesskey\"\n    secret_key: \"secret\"\n\n    endpoint_url: \"https://s3provider.example.com/bucket/path/\"\n    access_endpoint_url: \"https://my-custom-domain.example.com/path/\" #optional\n    is_default_primary: true\n</code></pre> <p>When using an external S3 provider, a custom <code>access_endpoint_url</code> can be provided, and the <code>bucket_name</code> need to be specified separately. This URL is used for direct access to WACZ files, and can be used to specify a custom domain to access the bucket.</p> <p>The <code>endpoint_url</code> should be provided in 'path prefix' form (with the bucket after the path), eg: <code>https://s3provider.example.com/bucket/path/</code>.</p> <p>Browsertrix will handle presigning S3 URLs so that WACZ files (and other data) can be accessed directly, using URLs of the form: <code>https://s3provider.example.com/bucket/path/to/files/crawl.wacz?signature...</code></p> <p>Since the local Minio service is not used, <code>minio_local: false</code> can be set to save resource in not deploying Minio.</p>"},{"location":"deploy/customization/#custom-access-endpoint-url","title":"Custom Access Endpoint URL","text":"<p>It may be useful to provide a custom access endpoint for accessing WACZ files and other data. If the <code>access_endpoint_url</code> is provided, it can be in either the 'virtual host' or 'path' form, while the <code>endpoint_url</code> should always be in path-prefix form.</p> <p>Here are two example of the addressing modes:</p>"},{"location":"deploy/customization/#virtual-host-vs-path-addressing-for-access-endpoints","title":"Virtual Host vs Path Addressing for Access Endpoints","text":"<p>Virtual host addressing: <pre><code>endpoint_url: https://s3provider.example.com/bucket/path/\naccess_endpoint_url: https://my-custom-domain.example.com/path/\naccess_addressing_style: virtual\n\n# Files loaded from: https://my-custom-domain.example.com/path/to/files/crawl.wacz?signature...\n</code></pre></p> <p>Path addressing: <pre><code>...\nendpoint_url: https://s3provider.example.com/bucket/path/\naccess_endpoint_url: https://my-custom-domain.example.com/bucket/path/\naccess_addressing_style: path\n\n# Files loaded from: https://my-custom-domain.example.com/bucket/path/to/files/crawl.wacz?signature...\n</code></pre></p> <p>Note that when using the local Minio for storage, path-style addressing is used automatically as the data is accessed via <code>/data/path/to/files</code>. Otherwise, virtual-style addressing is assumed as the default.</p>"},{"location":"deploy/customization/#storage-replicas","title":"Storage Replicas","text":"<p>It is possible to add one or more replica storage locations. If replica locations are enabled, all stored content in the application will be automatically replicated to each configured replica storage location in background jobs after being stored in the default primary storage. If replica locations are enabled, at least one must be set as the default replica location for primary backups. This is indicated with <code>is_default_replica: true</code>. If more than one storage location is configured, the primary storage must also be indicated with <code>is_default_primary: true</code>.</p> <p>For example, here is what a storage configuration with two replica locations, one in another bucket on the same local Minio S3 service as primary storage as well as another in an external S3 provider:</p> <pre><code>storages:\n  - name: \"default\"\n    type: \"s3\"\n    access_key: \"ADMIN\"\n    secret_key: \"PASSW0RD\"\n    bucket_name: btrix-data\n    access_endpoint_url: /data/\n\n    endpoint_url: \"http://local-minio.default:9000/\"\n    is_default_primary: true\n    # default for local minio is path\n    access_addressing_style: path\n\n  - name: \"replica-0\"\n    type: \"s3\"\n    access_key: \"ADMIN\"\n    secret_key: \"PASSW0RD\"\n    bucket_name: \"replica-0\"\n\n    endpoint_url: \"http://local-minio.default:9000/\"\n    is_default_replica: true\n    access_addressing_style: path\n\n  - name: \"replica-1\"\n    type: \"s3\"\n    access_key: \"accesskey\"\n    secret_key: \"secret\"\n    bucket_name: \"replica-1\"\n\n    endpoint_url: \"https://s3provider.example.com/bucket/path/\"\n    access_endpoint_url: \"https://bucket.my-custom-domain.example.com/path/\"\n    access_addressing_style: virtual\n</code></pre> <p>When replica locations are set, the default behavior when a crawl, upload, or browser profile is deleted is that the replica files are deleted at the same time as the file in primary storage. To delay deletion of replicas, set <code>replica_deletion_delay_days</code> in the Helm chart to the number of days by which to delay replica file deletion. This feature gives Browsertrix administrators time in the event of files being deleted accidentally or maliciously to recover copies from configured replica locations.</p> If you are specifying a custom Minio deployment running in the same Kubernetes cluster, be sure to update the network policy to allow access to your custom resource"},{"location":"deploy/customization/#horizontal-autoscaling","title":"Horizontal Autoscaling","text":"<p>Browsertrix also includes support for horizontal auto-scaling for both the backend and frontend pods. The auto-scaling will start a new pod when memory/cpu utilization reaches the thresholds.</p> <p>To use auto-scaling, the metrics-server cluster add-on is required. Many k8s provides include metrics server by default, others, like MicroK8S, make it available as an add-on.</p> <p>To enable auto-scaling, set <code>backend_max_replicas</code> and/or <code>frontend_max_replicas</code> to a value &gt;1.</p> <pre><code>backend_max_replicas: 2\n\nfrontend_max_replicas: 2\n</code></pre> <p>By default, the auto-scaling uses the following thresholds for deciding when to start a new pod can also be modified. The default values are:</p> <pre><code>backend_avg_cpu_threshold: 80\n\nbackend_avg_memory_threshold: 95\n\nfrontend_avg_cpu_threshold: 80\n\nfrontend_avg_memory_threshold: 95\n</code></pre>"},{"location":"deploy/customization/#email-smtp-server","title":"Email / SMTP Server","text":"<p>Browsertrix sends user invitations, password resets, background job failure notifications, and other important messages via email. The <code>email</code> setting can be used to configure the SMTP server used to send emails. To avoid email messages from Browsertrix being flagged as spam, be sure to use the same domain for <code>sender_email</code> and <code>reply_to_email</code>.</p>"},{"location":"deploy/customization/#customizing-email-templates","title":"Customizing Email Templates","text":"<p>It is also possible to custom the HTML/plain-text email templates that Browsertrix sends out with a custom <code>--set-file</code> parameter for <code>email.templates.&lt;TEMPLATE_NAME&gt;</code> pointing to an alternate template file. For example, to use a custom <code>invite.html</code> for the invite template, add:</p> <pre><code>helm upgrade --install btrix ... --set-file email.templates.invite=./invite.html\n</code></pre> <p>The list of available templates (and their default content) is available here</p> <p>The format of the template file is, for HTML emails:</p> <pre><code>Subject\n~~~\nHTML Content\n~~~\nText Content\n</code></pre> <p>or, for plain text emails:</p> <pre><code>Subject\n~~~\nText\n</code></pre> <p>The <code>~~~</code> is used to separate the sections. If only two sections are provided, the email template is treated as plain text, if three, an HTML email with plain text fallback is sent.</p>"},{"location":"deploy/customization/#signing-wacz-files","title":"Signing WACZ Files","text":"<p>Browsertrix has the ability to cryptographically sign WACZ files with Authsign. The <code>signer</code> setting can be used to enable this feature and configure Authsign.</p>"},{"location":"deploy/customization/#enable-open-registration","title":"Enable Open Registration","text":"<p>You can enable sign-ups by setting <code>registration_enabled</code> to <code>\"1\"</code>. Once enabled, your users can register by visiting <code>/sign-up</code>.</p>"},{"location":"deploy/customization/#inject-extra-javascript","title":"Inject Extra JavaScript","text":"<p>You can add a script to inject analytics, bug reporting tools, etc. into the frontend by setting <code>inject_extra</code> to script contents of your choosing. If present, it will be injected as a blocking script tag that runs when the frontend web app is initialized.</p> <p>For example, enabling analytics and tracking might look like this:</p> <pre><code>inject_extra: &gt;\n  const analytics = document.createElement(\"script\");\n  analytics.src = \"https://cdn.example.com/analytics.js\";\n  analytics.defer = true;\n\n  document.head.appendChild(analytics);\n\n  window.analytics = window.analytics\n    || function () { (window.analytics.q = window.analytics.q || []).push(arguments); };\n</code></pre> <p>Note that the script will only run when the web app loads, i.e. the first time the app is loaded in the browser and on hard refresh. The script will not run again upon clicking a link in the web app. This shouldn't be an issue with most analytics libraries, which should listen for changes to window history. If you have a custom script that needs to re-run when the frontend URL changes, you'll need to add an event listener for the <code>popstate</code> event.</p>"},{"location":"deploy/customization/#tracking-events","title":"Tracking events","text":"<p>Certain anonymized user interactions\u2014such as public collection views, downloads, and shares\u2014can be tracked for the purpose of collecting and analyzing metrics. To enable tracking these events, set <code>window.btrixEvent</code> in your <code>inject_extra</code> config to your custom track call. This should be a JavaScript function that conforms to the following type:</p> <pre><code>type btrixEvent = (\n  event: string,\n  extra?: {\n    props?: {\n      org_slug: string | null;\n      collection_slug?: string | null;\n      logged_in?: boolean;\n    };\n  },\n) =&gt; void;\n</code></pre> <p>Tracking is optional and will never expose personally identifiable information.</p>"},{"location":"deploy/customization/#local-network-access-policy-and-custom-services","title":"Local Network Access Policy and Custom Services","text":"<p>By default, Browsertrix configures the crawlers with a network policy that restricts access to internal Kubernetes resources, to prevent the crawler from snooping around the internal network. This should be fine for crawling public websites with the default configuration.</p> <p>However, you may want to provide access to an internal IP (for example, if crawling a site deployed on a local server) or another Kubernetes service (such as a custom Minio deployment)</p> <p>To provide access, you can extend the existing network policy 'egress' with the <code>crawler_network_policy_additional_egress</code> setting:</p> <p>For example, to allow the crawler to access the <code>10.0.0.1/32</code> IP block on port 80, and to pods that have a label <code>my-custom-minio</code> only on port 9000, add:</p> <pre><code>crawler_network_policy_additional_egress:\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.1/32\n    ports:\n      - port: 80\n        protocol: TCP\n\n  - to:\n    - podSelector:\n        matchLabels:\n          app: my-custom-minio\n\n    ports:\n      - port: 9000\n        protocol: TCP\n</code></pre> <p>Refer to the default networkpolicies.yaml for additional examples and the official Kubernetes documentation for Network Policies</p>"},{"location":"deploy/local/","title":"Local Deployment","text":"<p>To try out the latest release of Browsertrix on your local machine, you'll first need to have a working Kubernetes cluster.</p>"},{"location":"deploy/local/#installing-kubernetes","title":"Installing Kubernetes","text":"<p>Before running Browsertrix, you'll need to set up a running Kubernetes cluster.</p> <p>Today, there are numerous ways to deploy Kubernetes fairly easily, and we recommend trying one of the single-node options, which include Docker Desktop, microk8s, minikube, and k3s.</p> <p>The instructions below assume a local package manager for your platform (eg. <code>brew</code> for macOS, <code>choco</code> for Windows, etc...) is already installed.</p> <p>Cloning the repository at https://github.com/webrecorder/browsertrix is only needed to access additional configuration files.</p> <p>Here are some environment specific instructions for setting up a local cluster from different Kubernetes vendors:</p> Docker Desktop (recommended for macOS and Windows) <p>For macOS and Windows, we recommend testing out Browsertrix using Kubernetes support in Docker Desktop as that will be one of the simplest options.</p> <ol> <li> <p>Install Docker Desktop if not already installed.</p> </li> <li> <p>Under Settings &gt; Kubernetes, ensure Enable Kubernetes is checked.</p> </li> <li> <p>Restart Docker Desktop if asked, and wait for it to fully restart.</p> </li> <li> <p>Install Helm, with <code>brew install helm</code> (macOS) or <code>choco install kubernetes-helm</code> (Windows) or following some of the other install options</p> </li> </ol> MicroK8S (recommended for Ubuntu) <p>For Ubuntu and other Linux distros, we recommend using MicroK8S for both local deployment and production.</p> <ol> <li> <p>Install MicroK8s, by running <code>sudo snap install microk8s --classic</code> see more detailed instructions here or alternate installation instructions here.</p> </li> <li> <p>Install the following addons: <code>microk8s enable dns hostpath-storage registry helm3</code>. (For production, also add <code>ingress cert-manager</code> to the list of addons)</p> </li> <li> <p>Wait for add-ons to finish installing with <code>microk8s status --wait-ready</code></p> </li> </ol> <p>Note: microk8s comes with its own version helm, so you don't need to install it separately. Replace <code>helm</code> with <code>microk8s helm3</code> in the subsequent instructions below.</p> Minikube (Windows, macOS, or Linux) <ol> <li> <p>Install Minikube following installation instructions, eg. <code>brew install minikube</code>.    Note that Minikube also requires Docker or another container management system to be installed as well.</p> </li> <li> <p>Install Helm, with <code>!sh brew install helm</code> (macOS) or <code>choco install kubernetes-helm</code> (Windows) or following some of the other install options</p> </li> </ol> K3S (recommended for non-Ubuntu Linux) <ol> <li> <p>Install K3s as per the instructions</p> </li> <li> <p>Install Helm, with <code>brew install helm</code> (macOS) or <code>choco install kubernetes-helm</code> (Windows) or following some of the other install options</p> </li> <li> <p>Set <code>KUBECONFIG</code> to point to the config for K3S: <code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml</code> to ensure Helm will use the correct version.</p> </li> </ol>"},{"location":"deploy/local/#launching-browsertrix-with-helm-repository","title":"Launching Browsertrix with Helm Repository","text":"<p>Once you have a running Kubernetes cluster with Helm 3 installed using one of the options, you can add the Browsertrix Helm Repository:</p> <pre><code>helm repo add browsertrix https://docs.browsertrix.com/helm-repo/\n</code></pre> <p>and then install the latest version of the Browsertrix helm chart with:</p> <pre><code>helm upgrade --install btrix browsertrix/browsertrix\n</code></pre> <p>You can optionally specify a specific version with the <code>--version</code> flag:</p> <p></p> <pre><code>helm upgrade --install btrix browsertrix/browsertrix --version VERSION\n</code></pre> <p>The versions correspond to the available Release Tags</p>"},{"location":"deploy/local/#installing-from-github-release-directly","title":"Installing from GitHub Release Directly","text":"<p>Alternatively, you can also use Helm to install a specific version of Browsertrix directly from the latest GitHub release, if you don't wish to add the Helm repository</p> <p></p> <pre><code>helm upgrade --install btrix \\\nhttps://github.com/webrecorder/browsertrix/releases/download/VERSION/browsertrix-VERSION.tgz\n</code></pre> <p>However, the Helm repository option is recommended as it makes upgrading to the latest version easier.</p> MicroK8S <p>If using microk8s, the commands will be:</p> <pre><code>microk8s helm3 repo add browsertrix https://docs.browsertrix.com/helm-repo/\nmicrok8s helm3 upgrade --install btrix browsertrix/browsertrix\n</code></pre> <p>for the Helm repo option or, for a direct install:</p> <p></p> <pre><code>microk8s helm3 upgrade --install btrix \\\nhttps://github.com/webrecorder/browsertrix/releases/download/VERSION/browsertrix-VERSION.tgz\n</code></pre> <p>Note: Subsequent commands will also use <code>microk8s helm3</code> instead of <code>helm</code>.</p> <p>The default setup includes the full Browsertrix system, with frontend, backend api, db (via MongoDB), and storage (via Minio)</p> <p>An admin user with name <code>admin@example.com</code> and password <code>PASSW0RD!</code> will be automatically created.</p> <p>With Helm, additional YAML files can be added to further override previous settings.</p> <p>Some possible settings can be changed are found in chart/examples/local-config.yaml.</p> <p>For example, to change the default superadmin, uncomment the <code>superadmin</code> block in <code>local-config.yaml</code>, and then change the username (<code>admin@example.com</code>) and password (<code>PASSW0RD!</code>) to different values. (The admin username and password will be updated with each deployment). To change the local port, change <code>local_service_port</code> setting.</p> <p>You can then redeploy with these additional settings by running:</p> <p></p> <pre><code>helm upgrade --install btrix https://github.com/webrecorder/browsertrix/releases/download/VERSION/browsertrix-VERSION.tgz \\\n-f ./chart/examples/local-config.yaml\n</code></pre> <p>The above examples assumes running from a cloned Browsertrix repo, however the config file can be saved anywhere and specified with <code>-f &lt;extra-config.yaml&gt;</code>.</p>"},{"location":"deploy/local/#waiting-for-cluster-to-start","title":"Waiting for Cluster to Start","text":"<p>After running the helm command, you should see something like:</p> <pre><code>Release \"btrix\" does not exist. Installing it now.\nNAME: btrix\nLAST DEPLOYED: &lt;time&gt;\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>After that, especially on first run, it may take a few minutes for the Browsertrix cluster to start, as all images need to be downloaded locally.</p> <p>You can try running the following command to wait for all pods to be initialized: </p> <pre><code>kubectl wait --for=condition=ready pod --all --timeout=300s\n</code></pre> <p>The command will exit when all pods have been loaded, or if there is an error and it times out.</p> <p>If the command succeeds, you should be able to access Browsertrix by loading: http://localhost:30870/ in your browser.</p> Minikube (on macOS) <p>When using Minikube on a macOS, the port will not be 30870. Instead, Minikube opens a tunnel to a random port, obtained by running <code>minikube service browsertrix-cloud-frontend --url</code> in a separate terminal. Use the provided URL (in the format <code>http://127.0.0.1:&lt;TUNNEL_PORT&gt;</code>) instead.</p>"},{"location":"deploy/local/#debugging-pod-issues","title":"Debugging Pod Issues","text":"<p>If this command fails, you can also run <code>kubectl get pods</code> to see the status of each of the pods.</p> <p>There should be 4 pods listed: backend, frontend, minio, and mongodb. If any one is not ready for a while, something may be wrong.</p> <p>To get more details about why a pod has not started, run <code>kubectl describe &lt;podname&gt;</code> and see the latest status at the bottom.</p> <p>Often, the error may be obvious, such as \"failed to pull an image.\"</p> <p>If the pod is running, or previously ran, you can also get the logs from the container by running <code>kubectl logs &lt;podname&gt;</code>.</p> <p>The outputs of these commands are helpful when reporting an issue on GitHub.</p> Firewall rules (on RHEL/Fedora) <p>On Red Hat Enterprise Linux and derivatives, communication between pods might be blocked by firewalld. There are short guides on configuring the firewall for these systems in the k3s and Microk8s documentation.</p>"},{"location":"deploy/local/#updating-the-cluster","title":"Updating the Cluster","text":"<p>To update the cluster, for example to update to new version <code>NEWVERSION</code>, re-run the same command again, which will pull the latest images. In this way, you can upgrade to the latest release of Browsertrix. The upgrade will preserve the database and current archives.</p> <pre><code>helm upgrade --install btrix https://github.com/webrecorder/browsertrix/releases/download/NEWVERSION/browsertrix-NEWVERSION.tgz\n</code></pre>"},{"location":"deploy/local/#uninstalling","title":"Uninstalling","text":"<p>To uninstall, run <code>helm uninstall btrix</code>.</p> <p>By default, the database + storage volumes are not automatically deleted, run <code>helm upgrade ...</code> again to restart the cluster in its current state.</p> <p>If you are upgrading from a previous version, and run into issues with <code>helm upgrade ...</code>, we recommend uninstalling and then re-running upgrade.</p>"},{"location":"deploy/local/#deleting-all-data","title":"Deleting all Data","text":"<p>To fully delete all persistent data (db + archives) created in the cluster, run <code>kubectl delete pvc --all</code> after uninstalling.</p>"},{"location":"deploy/local/#deploying-for-local-development","title":"Deploying for Local Development","text":"<p>These instructions are intended for deploying the cluster from the latest releases published on GitHub. See setting up cluster for local development for additional customizations related to developing Browsertrix and deploying from local images.</p>"},{"location":"deploy/proxies/","title":"Configuring Proxies","text":"<p>Browsertrix can be configured to direct crawling traffic through dedicated proxy servers, allowing websites to be crawled from a specific geographic location regardless of where Browsertrix itself is deployed.</p> <p>The Browsertrix superadmin can configure which proxy servers are available to which organizations or if they are shared across all organizations, and users can choose from one of the available proxies in each crawl workflow. Users can also configure the default crawling proxy that will be used for the organization in organization-wide Crawling Defaults.</p> <p>This guide covers how to set up proxy servers for use with Browsertrix, as well as how to configure Browsertrix to make those proxies available to organizations.</p>"},{"location":"deploy/proxies/#proxy-configuration","title":"Proxy Configuration","text":"<p>Browsertrix supports crawling through HTTP and SOCKS5 proxies, including through a SOCKS5 proxy over an SSH tunnel. For more information on what is supported in the underlying Browsertrix Crawler, see the Browsertrix Crawler documentation.</p>"},{"location":"deploy/proxies/#obtain-an-ssh-key-pair","title":"Obtain an SSH Key-pair","text":"<p>To set up a proxy server to use with Browsertrix as SOCKS5 over SSH, you will need an SSH public key-pair and:</p> <ul> <li>The SSH public key configured on the remote machine</li> <li>The SSH private key configured in Browsertrix</li> <li>The public host key of the remote machine configured in Browsertrix (optional)</li> </ul> <p>We recommend creating a dedicated SSH key-pair for use with Browsertrix, as well as a dedicated user, e.g. <code>proxy-user</code>, and not reusing existing keys or users.</p> <p>For basic information on how to create a key-pair using <code>ssh-keygen</code>, see existing guides such as this one from DigitalOcean or this one from ssh.com. We recommend an ECDSA key-pair.</p> <p>We recommend securing the SSH connection for the proxy user to contain the following settings. This can be done by adding a file such as <code>/etc/ssh/sshd_config.d/99-ssh-proxy.conf</code> where <code>proxy-user</code> is the user connecting to the machine.</p> <pre><code>Match User proxy-user\n    AllowTcpForwarding yes\n    X11Forwarding no\n    AllowAgentForwarding no\n    ForceCommand /bin/false\n    PubkeyAuthentication yes\n    PasswordAuthentication no\n</code></pre>"},{"location":"deploy/proxies/#browsertrix-configuration","title":"Browsertrix Configuration","text":"<p>Proxies are configured in Browsertrix through a separate subchart, and can be configured in the <code>btrix-proxies</code> section of the main Helm chart (or local override file) for the Browsertrix deployment. Alternatively, they can be configured as a separate subchart.</p> <p>The proxy configuration will look like this, containing one or more proxy declarations.</p> <pre><code>#default_proxy: &lt;set for default proxy&gt;\n\nbtrix-proxies:\n  enabled: true\n  proxies:\n    - id: proxy-id-1\n      shared: true\n      label: My Proxy\n      description: Proxy hosted in for Browsertrix\n      country_code: US\n      url: ssh://proxy-user@ssh-proxy-host\n      ssh_host_public_key: &lt;host public key&gt;\n      ssh_private_key: &lt;private key&gt;\n\n    - id: proxy-id-2\n      shared: false\n      label: My SOCKS5 proxy\n      country_code: DE\n      url: socks5://username:password@proxy-host\n      ...\n</code></pre> <p>First, set <code>enabled</code> to <code>true</code> to enable proxies.</p> <p>Next, provide the details of each proxy server that you want to make available in the <code>proxies</code> list. Minimally, the <code>id</code>, <code>url</code> connection string, <code>label</code> name, and two-letter <code>country_code</code> must be set for each proxy.</p>"},{"location":"deploy/proxies/#ssh-proxies","title":"SSH Proxies","text":"<p>For SSH proxy servers,The <code>url</code> should be of the form <code>ssh://proxy-user@ssh-proxy-host</code>.  </p> <p>The <code>ssh_private_key</code> is required and is the private key of the key-pair created above.</p> <p>The <code>ssh_host_public_key</code> is recommended to help ensure a secure connection and can often be obtained by running: <code>ssh-keyscan dev.proxy-host -p 22</code> on the remote machine, assuming default SSH setup and hostname of <code>proxy-host</code>.</p> <p>Only key-based auth is supported for SSH proxies, password-based authentication is not supported.</p>"},{"location":"deploy/proxies/#socks5-proxies","title":"SOCKS5 Proxies","text":"<p>For SOCKS5 proxies, the <code>url</code> should be of the form <code>socks5://username:password@socks-proxy-host</code>.</p> <p>This method is to be used with dedicated SOCKS5 proxies (not over SSH), such as existing services that provide this feature.</p>"},{"location":"deploy/proxies/#shared-proxies","title":"Shared Proxies","text":"<p>The <code>shared</code> field on each proxy object defines if this proxy should be accessible to all organizations in a Browsertrix deployment that are allowed to access shared proxy. If false, the proxy must be added directly to each organization that will have access to the proxy.</p> <p>The proxy settings can be be configured in the super-admin UI by clicking the Edit Proxies dropdown option in the actions menu for each organization.</p>"},{"location":"deploy/proxies/#default-proxy","title":"Default Proxy","text":"<p>The <code>default_proxy</code> field in the root of the Helm values file can optionally be set to the <code>id</code> for one of the available proxies list. If set, the default proxy will be used for all crawls that do not have an alternate proxy set in the workflow configuration. This can be useful if Browsertrix is deployed on a private network and requires a proxy to access the outside world.</p> <p>This is a deployment-wide setting and is not shown to users, and is designed for admins to route all traffic through a designated proxy. Browsertrix will fail to start if the default proxy is not listed in the available proxies.</p>"},{"location":"deploy/proxies/#deployment","title":"Deployment","text":"<p>If <code>btrix-proxies</code> have been set in the main Helm chart or a local override file for your Browsertrix deployment, proxies will be enabled on next deploy of the Browsertrix helm chart. For instance, if the proxy configuration is located in a local override file <code>local.yaml</code>, you can use the following Helm command to redeploy Browsertrix with the proxy configuration:</p> <pre><code>helm upgrade --install -f ./chart/values.yaml -f ./chart/local.yaml btrix ./chart/\n</code></pre>"},{"location":"deploy/proxies/#deploying-with-proxies-via-subchart","title":"Deploying with Proxies via subchart","text":"<p>Proxies can alternatively be configured with a separate proxies subchart.</p> <p>This allows proxies to be updated without having to redeploy all of Browsertrix.</p> <p>A separate proxies YAML file should contain just the <code>proxies</code> key:</p> <pre><code>proxies:\n  - id: proxy-id-1\n    shared: true\n    label: My Proxy\n    description: Proxy hosted in for Browsertrix\n    country_code: US\n    url: ssh://proxy-user@ssh-proxy-host\n    ssh_host_public_key: &lt;host public key&gt;\n    ssh_private_key: &lt;private key&gt;\n\n  - id: proxy-id-2\n    shared: false\n    label: My SOCKS5 proxy\n    country_code: DE\n    url: socks5://username:password@proxy-host\n</code></pre> <p>If the above YAML is placed in <code>proxies.yaml</code>, the subchart can be deployed with the following command and Browsertrix will pick up the updated proxies:</p> <pre><code>helm upgrade --install -f ./chart/proxies.yaml proxies ./chart/proxies/\n</code></pre>"},{"location":"deploy/proxies/#github-release-for-subchart","title":"GitHub Release for subchart","text":"<p>The above layout assumes a local copy of the Browsertrix repo.</p> <p>The proxies subchart can also be deployed from the latest GitHub release via:</p> <pre><code>helm upgrade --install proxies https://github.com/webrecorder/browsertrix/releases/download/RELEASE/btrix-proxies-VERSION.tgz\n</code></pre> <p>where <code>RELEASE</code> is the Browsertrix release and <code>VERSION</code> is the version of the proxies chart.</p> <p>See the Browsertrix releases page for the latest available versions.</p>"},{"location":"deploy/remote/","title":"Remote: Self-Hosted and Cloud","text":"<p>For remote and hosted deployments (both on a single machine or in the cloud), the only requirement is to have a designed domain and (strongly recommended, but not required) second domain for signing web archives. </p> <p>We are also experimenting with Ansible playbooks for cloud deployment setups.</p> <p>The production deployments also allow using an external mongodb server, and/or external S3-compatible storage instead of the bundled minio.</p>"},{"location":"deploy/remote/#single-machine-deployment-with-microk8s","title":"Single Machine Deployment with MicroK8S","text":"<p>For a single-machine remote deployment, we recommend using MicroK8s.</p> <ol> <li> <p>Install MicroK8S, as suggested in the local deployment guide and ensure the <code>ingress</code> and <code>cert-manager</code> addons are also enabled.</p> </li> <li> <p>Copy <code>cp ./chart/examples/microk8s-hosted.yaml ./chart/my-config.yaml</code> to make local changes.</p> </li> <li> <p>Set the <code>ingress.host</code>, <code>ingress.cert_email</code> and <code>signing.host</code> fields in <code>./chart/my-config.yaml</code> to your host and domain</p> </li> <li> <p>Set the super-admin username and password, and mongodb username and password in <code>./chart/my-config.yaml</code></p> </li> <li> <p>Run with:</p> <pre><code>helm upgrade --install -f ./chart/values.yaml -f ./chart/my-config.yaml \\\n  btrix ./chart/\n</code></pre> </li> </ol>"},{"location":"deploy/remote/#single-machine-deployment-with-k3s","title":"Single Machine Deployment with k3s","text":"<p>Another option for a single-machine remote deployment is k3s</p> <ol> <li> <p>Install K3s, as suggested in the local deployment guide. Make sure to disable traefik which can be done by adding <code>--no-deploy traefik</code> to the <code>systemd</code> unit when installing k3s</p> </li> <li> <p>Install <code>nginx-ingress</code> with:</p> <pre><code>helm upgrade --install ingress-nginx ingress-nginx \\\n  --repo https://kubernetes.github.io/ingress-nginx \\\n  --namespace ingress-nginx --create-namespace\n</code></pre> </li> <li> <p>Install <code>cert-manager</code>. We recommend installing <code>cert-manager</code> through Jetpack, like so: </p> <pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nhelm repo update jetstack\nhelm upgrade --install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.12.0 \\\n  --set installCRDs=true\n</code></pre> </li> <li> <p>Copy <code>cp ./chart/examples/k3s-hosted.yaml ./chart/my-config.yaml</code> to make local changes.</p> </li> <li> <p>Set the <code>ingress.host</code>, <code>ingress.cert_email</code> and <code>signing.host</code> fields in <code>./chart/my-config.yaml</code> to your host and domain</p> </li> <li> <p>Set the super-admin username and password, and mongodb username and password in <code>./chart/my-config.yaml</code></p> </li> <li> <p>Run with:</p> <pre><code>helm upgrade --install -f ./chart/values.yaml -f ./chart/my-config.yaml \\\n  btrix ./chart/\n</code></pre> </li> </ol>"},{"location":"deploy/remote/#using-custom-storage","title":"Using Custom Storage","text":"<p>If you would like to use existing external storage, such an existing S3-compatible storage, also set the default storage, for example:</p> <pre><code>minio_local: false\n\nstorages:\n  - name: \"default\"\n    access_key: &lt;access key&gt;\n    secret_key: &lt;secret key&gt;\n\n    endpoint_url: \"https://s3.&lt;region&gt;.amazonaws.com/bucket/path/\"\n</code></pre> <p>Note that this setup is not limited to Amazon S3, but should work with any S3-compatible storage service.</p>"},{"location":"deploy/remote/#using-custom-mongodb","title":"Using Custom MongoDB","text":"<p>If you would like to use an externally hosted MongoDB, you can add the following config to point to a custom MongoDB instance.</p> <p>The <code>db_url</code> should follow the MongoDB Connection String Format which should include the username and password of the remote instance.</p> <pre><code>mongo_local: false\n\nmongo_auth:\n  db_url: mongodb+srv://...\n</code></pre>"},{"location":"deploy/remote/#cloud-deployment","title":"Cloud Deployment","text":"<p>There are also many ways to deploy Browsertrix on various cloud providers.</p> <p>To simplify this process, we are working on Ansible playbooks for setting up Browsertrix on commonly used infrastructure.</p>"},{"location":"deploy/remote/#ansible-deployment","title":"Ansible Deployment","text":"<p>Ansible makes the initial setup and configuration of your Browsertrix instance automated and repeatable. </p> <p>To use, you will need to install Ansible on your control computer and then you can use these to deploy to Browsertrix on remote and cloud environments.</p> <p>Currently, we provide playbooks for the following tested environments:</p> <ul> <li>DigitalOcean</li> <li>Microk8s</li> <li>k3s</li> </ul>"},{"location":"deploy/admin/org-import-export/","title":"Org Import &amp; Export","text":"<p>This guide covers exporting an organization from Browsertrix and optionally importing it into another Browsertrix cluster.</p> <p>Both export and import are two-step processes, which involve:</p> <ol> <li>Copying an organization's crawl, upload, and profile files in object storage</li> <li>Copying an organization's database information via a portable JSON file</li> </ol>"},{"location":"deploy/admin/org-import-export/#export","title":"Export","text":""},{"location":"deploy/admin/org-import-export/#export-files","title":"Export files","text":"<p>An organization's files are co-located within a \"directory\" in the S3 bucket being used for storage. This makes it possible to recursively copy all of the files in their original logical structure using tools such as the <code>aws s3</code> command-line interface or <code>rclone</code>, e.g.:</p> <pre><code>aws s3 cp s3://current-bucket/&lt;org-id&gt; /path/to/local/directory/&lt;org-id&gt; --recursive --endpoint=https://ams3.digitaloceanspaces.com\n</code></pre> <p>It is important to retain the directory structure to re-import an organization's files into another Browsertrix cluster later, as some assets such as browser profiles and uploads  have \"subdirectory\" prefixes.</p> <p>Note</p> <p>Browsertrix uses S3-compatible object storage to manage files. In object storage systems, all files are stored flat in the underlying system but presented in logical \"directories\" based on file prefixes for user convenience.</p> <p>When we speak of a \"directory\" in an S3 bucket in this guide, we are referring to a consistent file prefix, in this case an organization's ID.</p> <p>When files are exported from an S3 bucket to a local filesystem such as a laptop or desktop computer, these logical \"directories\" will turn into folders in the local filesystem.</p> <p>To move an organization from one Browsertrix cluster to another, sync the org id \"directory\" from one S3 bucket to another directly, as there is no need to download files locally as an intermediary step.</p>"},{"location":"deploy/admin/org-import-export/#export-database-information","title":"Export Database Information","text":"<p>To generate a portable JSON representation of an org's database information, use the <code>GET /api/orgs/&lt;org-id&gt;/export/json</code> API endpoint and save the returned JSON to a file, e.g.:</p> <pre><code>curl -H \"Content-type: application/json\" -H \"Authorization: Bearer &lt;jwt token&gt;\" https://app.browsertrix.com/api/orgs/&lt;org-id&gt;/export/json -o org-export.json\n</code></pre> <p>This endpoint is available to superusers only.</p>"},{"location":"deploy/admin/org-import-export/#import","title":"Import","text":""},{"location":"deploy/admin/org-import-export/#import-files","title":"Import Files","text":"<p>If an organization's files have already been copied to the S3 bucket configured in the new cluster, skip this step. Otherwise, use a tool such as the <code>aws s3</code> command-line interface or <code>rclone</code> to sync the local organization directory to the new bucket, being careful to retain the org ID directory and logical structure within, e.g.:</p> <pre><code>aws s3 cp /path/to/local/directory/&lt;org-id&gt; s3://new-bucket/&lt;org-id&gt; --recursive\n</code></pre>"},{"location":"deploy/admin/org-import-export/#import-database-information","title":"Import Database Information","text":"<p>To import an organization from a JSON export, use the <code>POST /api/orgs/import/json</code> API endpoint, adding the JSON export file to the request as a stream, e.g.:</p> <pre><code>curl -X POST -H \"Content-Type: application/octet-stream\" -H \"Authorization: Bearer &lt;jwt token&gt;\" --data-binary \"@org-export.json\" https://app.browsertrix.com/api/orgs/import/json\n</code></pre> <p>This endpoint is available to superusers only.</p> <p>The organization name must not already exist in the new cluster or the import API endpoint will fail and return a <code>400</code> status code.</p> <p>In addition to importing the organization and its constituent parts such as workflows, crawls, uploads, profiles, and collections, the import process will also recreate any users from the original organization that do not exist on the new cluster. These users are given the same roles in the imported organization and retain their names and email addresses. If a user account already exists on the new cluster with the same email address, that user is given their original role in the imported organization. References to user IDs throughout the organization are updated on import for any newly created users.</p> <p>Newly created imported users are given a new secure random password. Prior to logging in on the new cluster for the first times, users will need to request a password reset from the login screen and follow the directions in the resulting email to create a new password.</p>"},{"location":"deploy/admin/org-import-export/#storage-configuration","title":"Storage Configuration","text":"<p>The storage name referenced in the organization and files to be imported must match the storage configuration name for primary storage in the newly created cluster.</p> <p>If the storage name and configuration details are identical in the original and new clusters, no additional steps need to be taken.</p> <p>If the primary storage for the new cluster uses a different name than the original cluster, update the storage references during import by passing the <code>storageName</code> query parameter to the import API endpont, e.g.:</p> <pre><code>curl -X POST -H \"Content-type: application/json\" -H \"Authorization: Bearer &lt;jwt token&gt;\" --data-binary \"@org-export.json\" https://app.browsertrix.com/api/orgs/import/json?storageName=newname\n</code></pre>"},{"location":"deploy/admin/org-import-export/#database-versions","title":"Database Versions","text":"<p>By default, the import API endpoint will fail and return a <code>400</code> status code if the database version in the imported JSON differs from the database version of the new cluster.</p> <p>To ignore this check, pass the <code>ignoreVersion</code> query parameter with a true value to the import API endpoint, e.g.:</p> <pre><code>curl -X POST -H \"Content-type: application/json\" -H \"Authorization: Bearer &lt;jwt token&gt;\" --data-binary \"@org-export.json\" https://app.browsertrix.com/api/orgs/import/json?ignoreVersion=true\n</code></pre> <p>If the JSON export is from an earlier database version than the cluster the org is being imported into, re-run migrations from the version in the JSON export after importing the org. To do this, re-install the application with helm, setting <code>rerun_from_migration</code> in the helm chart to the database version specified in the JSON export.</p>"},{"location":"deploy/admin/upgrade-notes/","title":"Upgrade Notes","text":"<p>Some Browsertrix releases include long-running data migrations that may need to be monitored. This guide covers important information for such releases.</p>"},{"location":"deploy/admin/upgrade-notes/#browsertrix-114","title":"Browsertrix 1.14","text":"<p>Browsertrix 1.14, which introduces public collections, has several data migrations which affect crawl and upload objects as well as their pages.</p> <p>Migration 0042 in particular annotates all crawl pages in the database with information which is used to optimize loading times for crawl and collection replay. Because it must iterate through all crawl pages, this process can take a long time in deployments with many crawls and pages.</p> <p>In order to keep this optimization from blocking deployment, migration 0042 starts a parallelized background job that migrates the important data.</p> <p>If this background job fail for any reason, the superadmin will receive a background job failure notification. The status of the background job can also be checked or retried at any time using superadmin-only background job API endpoints as needed:</p> <ul> <li>List all background jobs: <code>GET /orgs/all/jobs</code></li> <li>Get background job: <code>GET /orgs/all/jobs/{job_id}</code></li> <li>Retry background job: <code>POST /orgs/all/jobs/{job_id}/retry</code></li> </ul> <p>For more details on these and other available API endpoints, consult the Browsertrix API documentation.</p>"},{"location":"deploy/ansible/digitalocean/","title":"DigitalOcean","text":"<p>Playbook Path: ansible/playbooks/install_microk8s.yml</p> <p>This playbook provides an easy way to install Browsertrix on DigitalOcean. It automatically sets up Browsertrix with LetsEncrypt certificates.</p>"},{"location":"deploy/ansible/digitalocean/#requirements","title":"Requirements","text":"<p>To run this ansible playbook, you need to:</p> <ul> <li>Have a DigitalOcean Account where this will run.</li> <li>Create a DigitalOcean API Key which will need to be set in your terminal sessions environment variables <code>export DO_API_TOKEN</code> </li> <li><code>doctl</code> command line client configured (run <code>doctl auth init</code>)</li> <li>Create a DigitalOcean Spaces API Key which will also need to be set in your terminal sessions environment variables, which should be set as <code>DO_AWS_ACCESS_KEY</code> and <code>DO_AWS_SECRET_KEY</code></li> <li>Configure a DNS A Record and CNAME record.</li> <li>Have a working Python and pip configuration through your OS Package Manager</li> </ul>"},{"location":"deploy/ansible/digitalocean/#install","title":"Install","text":"<ol> <li> <p>Clone the repo: <pre><code>git clone https://github.com/webrecorder/browsertrix.git\ncd browsertrix\n</code></pre></p> </li> <li> <p>Install the Dependencies through pipenv <pre><code>cd ansible\npip install pipenv\npipenv install\npipenv shell\n</code></pre></p> </li> <li> <p>Look at the configuration options and modify them or pass them as extra variables as shown below. If you haven't configured <code>kubectl</code>, please enable the <code>configure_kube</code> option </p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook do_setup.yml -e project_name=\"your-project\" -e superuser_email=\"you@yourdomain.com\" -e domain=\"yourdomain.com\"\n</code></pre></p> </li> </ol> <p>You may optionally configure these command line parameters through the group_vars file</p>"},{"location":"deploy/ansible/digitalocean/#upgrading","title":"Upgrading","text":"<ol> <li> <p>Run <code>git pull</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook do_setup.yml -e project_name=\"your-project\" -e superuser_email=\"you@yourdomain.com\" -e domain=\"yourdomain.com\" -t helm_upgrade\n</code></pre></p> </li> </ol>"},{"location":"deploy/ansible/digitalocean/#uninstall","title":"Uninstall","text":"<p>You can tear down your deployment through ansible as well. By default ansible will dump all the databases into your DO space. You can configure an option to disable this feature. </p> <pre><code>ansible-playbook playbooks/do_teardown.yml -e project_name=\"your-project\" -e superuser_email=\"you@yourdomain.com\" -e domain=\"yourdomain.com\"\n</code></pre>"},{"location":"deploy/ansible/k3s/","title":"K3S","text":"<p>Playbook Path: ansible/playbooks/install_k3s.yml</p> <p>This playbook provides an easy way to install Browsertrix on a Linux box (tested on Rocky Linux 9). It automatically sets up Browsertrix with Let's Encrypt certificates.</p>"},{"location":"deploy/ansible/k3s/#requirements","title":"Requirements","text":"<p>To run this ansible playbook, you need to:</p> <ul> <li>Have a server / VPS where Browsertrix will run.</li> <li>Configure a DNS A Record to point at your server's IP address.</li> <li>Make sure SSH is working, with a sudo user: <code>ssh your-user@your-domain</code></li> <li> <p>Install Ansible on your local machine (the control machine).</p> </li> <li> <p>Clone the repo: <pre><code>git clone https://github.com/webrecorder/browsertrix.git\ncd browsertrix\n</code></pre></p> </li> <li> <p>Optional: Create a copy of the [inventory directory] and name it what you like (alternatively edit the sample files in place) <pre><code>cp -r ansible/inventory/sample-k3s ansible/inventory/my-deployment\n</code></pre></p> </li> <li> <p>Look at the configuration options and modify them to match your setup </p> </li> <li> <p>Change the hosts IP address in your just created inventory</p> </li> <li> <p>You may need to make modifications to the playbook itself based on your configuration. The playbook lists sections that can be removed or changed based on whether you'd like to install a multi-node or single-node k3s installation for your Browsertrix deployment. By default the playbook assumes you'll run in a single-node environment deploying directly to <code>localhost</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/my-deployment/hosts.ini install_k3s.yml\n</code></pre></p> </li> </ul>"},{"location":"deploy/ansible/k3s/#upgrading","title":"Upgrading","text":"<ol> <li> <p>Run <code>git pull</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/hosts install_k3s.yml -t helm_upgrade\n</code></pre></p> </li> </ol>"},{"location":"deploy/ansible/microk8s/","title":"Microk8s","text":"<p>Playbook Path: ansible/playbooks/install_microk8s.yml</p> <p>This playbook provides an easy way to install Browsertrix on Ubuntu (tested on Jammy Jellyfish) and RedHat 9 (tested on Rocky Linux 9). It automatically sets up Browsertrix with Letsencrypt certificates.</p>"},{"location":"deploy/ansible/microk8s/#requirements","title":"Requirements","text":"<p>To run this ansible playbook, you need to:</p> <ul> <li>Have a server / VPS where browsertrix will run.</li> <li>Configure a DNS A Record to point at your server's IP address.</li> <li>Make sure you can ssh to it, with a sudo user: ssh @ <li>Install Ansible on your local machine (the control machine).</li> <p>Note</p> <p>Ansible requires an SSH key with no password. You cannot use a passphrase.     Sudo must similarly be available without a passphrase for ansible to work</p> Info <pre><code>You will need to install `acl` on the target Ansible machine to set permissions: \n`sudo apt-get install acl`\n</code></pre>"},{"location":"deploy/ansible/microk8s/#install","title":"Install","text":"<ol> <li> <p>Clone the repo: <pre><code>git clone https://github.com/webrecorder/browsertrix.git\ncd browsertrix/ansible\n</code></pre></p> </li> <li> <p>Look at the configuration options and modify them or pass them as extra variables as shown below. </p> </li> <li> <p>Add your IP address above to a new file called [inventory/hosts]</p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/hosts install_microk8s.yml -e host_ip=\"1.2.3.4\" -e domain_name=\"yourdomain.com\" -e your_user=\"your_vps_admin_user\"\n</code></pre></p> </li> </ol>"},{"location":"deploy/ansible/microk8s/#upgrading","title":"Upgrading","text":"<ol> <li> <p>Run <code>git pull</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/hosts install_microk8s.yml -e host_ip=\"1.2.3.4\" -e domain_name=\"yourdomain.com\" -t helm_upgrade\n</code></pre></p> </li> </ol>"},{"location":"develop/","title":"Developing Browsertrix","text":""},{"location":"develop/#local-development","title":"Local Development","text":"<p>Get the latest Browsertrix source using git:</p> <pre><code>git clone https://github.com/webrecorder/browsertrix.git\n</code></pre> <p>To develop Browsertrix, the system must first be deployed locally in a Kubernetes cluster. The deployment can then be further customized for local development.</p>"},{"location":"develop/#source-code","title":"Source Code","text":"<p>Browsertrix consists of a Python-based backend and TypeScript-based frontend.</p>"},{"location":"develop/#backend","title":"Backend","text":"<p>The backend is an API-only system, using the FastAPI framework. Latest API docs can be viewed in the browser by adding <code>/api/redoc</code> to the URL of a running cluster (ex: <code>http://localhost:30870/api/redoc</code> when running locally on port <code>30870</code>.)</p> <p>At this time, the backend must be deployed in the Kubernetes cluster.</p>"},{"location":"develop/#frontend","title":"Frontend","text":"<p>The frontend UI is implemented in TypeScript, using the Lit framework and Shoelace component library.</p> <p>The static build of the frontend is bundled with nginx, but the frontend can be deployed locally in dev mode against an existing backend.</p> <p>See Developing the Frontend UI for more details.</p>"},{"location":"develop/#contributing","title":"Contributing","text":"<p>Browsertrix is planned and developed on GitHub: https://github.com/webrecorder/browsertrix. We welcome pull requests that contribute towards fixing bugs and feature enhancements.</p> <p>Check out our project board to see current and upcoming features that the Webrecorder team is working on.</p>"},{"location":"develop/docs/","title":"Writing Documentation","text":"<p>Our documentation is built with Material for MkDocs and configured via <code>mkdocs.yml</code> in the project root.</p> <p>The docs can be found in the <code>frontend/docs</code> subdirectory.</p>"},{"location":"develop/docs/#installation","title":"Installation","text":"<p>First, change your working directory to <code>frontend/docs</code>. Then, to run the docs locally:</p> pippipxuvx <p>Install Material for MkDocs:</p> <pre><code>pip install mkdocs-material\n</code></pre> <p>Start the docs development server:</p> <pre><code>mkdocs serve\n</code></pre> <p>Install Material for MkDocs:</p> <pre><code>pipx install mkdocs-material --include-deps\n</code></pre> <p>Start the docs development server:</p> <pre><code>mkdocs serve\n</code></pre> <p>Install and start the docs development server:</p> <pre><code>uvx --with mkdocs-material --with mkdocs-redirects mkdocs serve\n</code></pre> <p>You can now view a local version of the docs at localhost:8000.</p> Differences between self-hosted and Webrecorder hosted docs <p>The docs available online at docs.browsertrix.com may differ from the main branch of github.com/webrecorder/browsertrix. The online documentation corresponds to the latest hosted Browsertrix production release.</p>"},{"location":"develop/docs/#adding-new-pages","title":"Adding New Pages","text":"<ol> <li>Create a Markdown file in the directory of choice</li> <li>Add the newly created Markdown file to the <code>nav</code> value under the subsection as defined by the file's location in <code>mkdocs.yml</code>.</li> </ol>"},{"location":"develop/docs/#adding-icons","title":"Adding Icons","text":"<p>We typically use the Bootstrap icon set with our projects. This set is quite expansive, and we don't add the entire set into our docs folder as most icons go unused. If you wish to use an icon when writing documentation to refer to an icon present in part of the app, you may have to download the SVG file and add it to the repo.</p> <p>Icons are placed in the <code>docs/overrides/.icons/iconsetname/icon-name.svg</code> directory, and can be added in markdown files as <code>:iconsetname-icon-name:</code> accordingly. After adding icons to the folder, MKDocs must be restarted. For more information, see the Material for MKDocs page on Changing the logo and icons.</p>"},{"location":"develop/docs/#docs-style-guide","title":"Docs Style Guide","text":""},{"location":"develop/docs/#american-english","title":"American English","text":"<p>Webrecorder is a global team but we use American English when writing documentation and in-app copy. Some basic rules to follow are:</p> <ol> <li>Swap the <code>s</code> for a <code>z</code> in words like categorize and pluralize.</li> <li>Remove the <code>u</code> from words like color and honor.</li> <li>Swap <code>tre</code> for <code>ter</code> in words like center.</li> <li>Numbers should be formatted with commas for separation of values, using periods to denote decimals (e.g: 3,153.89, not 3 153,89).</li> </ol>"},{"location":"develop/docs/#oxford-commas","title":"Oxford Commas","text":"<p>In a list of three or more items, the list item proceeding the word \"and\" should have a comma placed after it clarifying that the final item in the list is not a part of the previous item.</p>"},{"location":"develop/docs/#example","title":"Example","text":"Use Don't use One, two, three, and four. One, two, three and four. Charles, Ada, and Alan. Charles, Ada and Alan."},{"location":"develop/docs/#capitalization-of-concepts-and-tools","title":"Capitalization of Concepts and Tools","text":"<p>Webrecorder has a number of common nouns that we use in our products. Examples include: archived items, crawl workflows, browser profiles, collections, and organizations. Because these are concepts and not specific instances of each concept, do not capitalize them unless they are at the start of a sentence.</p>"},{"location":"develop/docs/#example_1","title":"Example","text":"<p>When starting a sentence:</p> <p>Archived items consist of one or more...</p> <p>In the middle of a sentence:</p> <p>...they are omitted from the archived items list page...</p> <p>Webrecorder's software packages are all proper nouns and should always be capitalized. Examples include: Browsertrix, ReplayWeb.page, ArchiveWeb.Page, and PYWB. Specific pages such as the Archived Items page should also be capitalized as they are not referencing the concept of archived items and are instead referencing the page in question that happens to share the same name.</p>"},{"location":"develop/docs/#be-concise-avoid-you-statements","title":"Be Concise, Avoid \"You\" Statements","text":"<p>Generally, people don't want to have to read documentation. When writing, try to explain concepts simply and with clear objective language. Do not use \"we\" to refer to communication between the author and the reader, use \"we\" to refer to Webrecorder. \"You can\" or \"you may\" can be used, but preferably when giving supplemental advice and generally not when providing instructions that should be followed to achieve a successful outcome. Otherwise, avoid spending time referring to the reader, instead tell them what they should know.</p>"},{"location":"develop/docs/#example_2","title":"Example","text":"<p>If you want to do x, you can click on y.</p> <p>Can often be shortened to:</p> <p>To do x, click on y.</p>"},{"location":"develop/docs/#acronyms","title":"Acronyms","text":"<p>Avoid using acronyms when reuse is not frequent enough to warrant space savings. When acronyms must be used, spell the full phrase first and include the acronym in parentheses <code>()</code> the first time it is used in each document. This can be omitted for extremely common acronyms such as \"URL\" or \"HTTP\".</p>"},{"location":"develop/docs/#example_3","title":"Example","text":"<p>When running in a Virtual Machine (VM), use the....</p>"},{"location":"develop/docs/#headings","title":"Headings","text":"<p>All headings should be in MLA style title case.</p>"},{"location":"develop/docs/#example_4","title":"Example","text":"<p>Indiana Jones and the Raiders of the Lost Ark</p>"},{"location":"develop/docs/#referencing-features-and-their-options","title":"Referencing Features and Their Options","text":"<p>Controls with multiple options should have their options referenced as <code>in-line code blocks</code>.</p> <p>Setting names referenced outside of a heading should be Title Cased and set in italics.</p> <p>Actions with text (buttons in the app) should also be Title Cased and set in italics.</p>"},{"location":"develop/docs/#example_5","title":"Example","text":"<p>Sets the day of the week for which crawls scheduled with a <code>Weekly</code> Frequency will run.</p>"},{"location":"develop/docs/#manual-word-wrapping","title":"Manual Word Wrapping","text":"<p>Do not manually wrap words by adding newlines when writing documentation.</p>"},{"location":"develop/docs/#code-block-syntax-highlighting","title":"Code Block Syntax Highlighting","text":"<p>Tag the language to be used for syntax highlighting.</p>"},{"location":"develop/docs/#example_6","title":"Example","text":"<pre><code>```markdown\nexample markdown code block text\n```\n</code></pre> <p>For in-line code blocks, syntax highlighting should be added for all code-related usage by adding <code>#!language</code> to the start of all in-line code blocks. This is not required for paths or simply highlighting important text using in-line code blocks.</p>"},{"location":"develop/docs/#example_7","title":"Example","text":"<pre><code> `#!python range()`\n</code></pre> <p>Renders to: <code>range()</code></p>"},{"location":"develop/docs/#paid-features","title":"Paid features","text":"<p><code>Paid Feature</code></p> <p>Some features of Browsertrix only pertain to those paying for the software on a hosted plan. Denote these with the following:</p> <pre><code>`Paid Feature`{ .badge-green }\n</code></pre>"},{"location":"develop/docs/#admonitions","title":"Admonitions","text":"<p>We use Admonitions in their collapsed state to offer additional context or tips that aren't relevant to all users reading the section. We use standard un-collapsible ones when we need to call attention to a specific point.</p> <p>There are a lot of different options provided by Material for MkDocs \u2014 So many in fact that we try to pair down their usage into the following categories.</p> Note <p>The default call-out, used to highlight something if there isn't a more relevant one \u2014 should generally be expanded by default but can be collapsible by the user if the note is long.</p> <p>Tip: May have a title stating the tip or best practice</p> <p>Used to highlight a point that is useful for everyone to understand about the documented subject \u2014 should be expanded and kept brief.</p> Info: Must have a title describing the context under which this information is useful <p>Used to deliver context-based content such as things that are dependant on operating system or environment \u2014 should be collapsed by default.</p> Example: Must have a title describing the content <p>Used to deliver additional information about a feature that could be useful in a specific circumstance or that might not otherwise be considered \u2014 should be collapsed by default.</p> Question: Must have a title phrased in the form of a question <p>Used to answer frequently asked questions about the documented subject \u2014 should be collapsed by default.</p> <p>Warning: Must have a title stating the warning</p> <p>Used to deliver important information \u2014 should always be expanded.</p> <p>Danger: Must have a title stating the warning</p> <p>Used to deliver information about serious unrecoverable actions such as deleting large amounts of data or resetting things \u2014 should always be expanded.</p>"},{"location":"develop/emails/","title":"Developing Email Templates","text":"<p>The email templating engine is built using React Email. It includes a set of components and utilities for building email templates, as well as a tool for viewing and testing emails locally.</p> <p>To view and edit email templates, you can run the React Email development server:</p> <p><pre><code>cd emails\n</code></pre> <pre><code>yarn install\n</code></pre> <pre><code>yarn dev\n</code></pre></p> <p>You can then view the email templates in your browser at localhost:3000.</p> <p>Templates themselves are located in the <code>emails</code> directory.</p> <p>You can also send test emails from the development server using a service provided by Resend with the Send button in the top right corner when viewing an email template.</p>"},{"location":"develop/emails/#testing-email-sending-from-browsertrix","title":"Testing Email Sending from Browsertrix","text":"<p>Email sending can be tested locally with a tool such as smtp4dev.</p> <p>If you have previously deployed the Browsertrix backend, you'll need to make some changes to your <code>chart/local.yaml</code>:</p> <ol> <li>Update your <code>chart/local.yaml</code> to include the new service:   <pre><code># use version specified in values.yaml, uncomment to use :latest release instead\nbackend_image: docker.io/webrecorder/browsertrix-backend:latest\nemails_image: docker.io/webrecorder/browsertrix-emails:latest\nfrontend_image: docker.io/webrecorder/browsertrix-frontend:latest\n\n# overrides to use existing images in local Docker, otherwise will pull from repository\nbackend_pull_policy: \"Never\"\nemails_pull_policy: \"Never\"\nfrontend_pull_policy: \"Never\"\n</code></pre></li> <li>If you'd like to test sending emails locally rather than just viewing emails in pod logs without using an external e-mail service, you can use the smtp4dev Docker image locally (see Using smtp4dev). To do so, update your <code>chart/local.yaml</code> to include email sending options:   <pre><code>email:\n  smtp_host: \"host.docker.internal\"\n  smtp_port: 2525\n  sender_email: example@example.com\n  password: password\n  reply_to_email: example@example.com\n  use_tls: false\n</code></pre></li> <li>Build the updated backend and new emails images:   <pre><code>./scripts/build-backend.sh\n</code></pre> <pre><code>./scripts/build-emails.sh\n</code></pre></li> <li>Deploy the changes you've made:   <pre><code>helm upgrade --install -f ./chart/values.yaml -f ./chart/local.yaml btrix ./chart/\n</code></pre></li> </ol>"},{"location":"develop/emails/#using-smtp4dev","title":"Using smtp4dev","text":"<p>If you're using Docker Desktop or a similar Docker-compatible Kubernetes runtime (e.g. OrbStack), you can use a command such as this to start up smtp4dev: <pre><code>docker run --rm -it -p 5000:80 -p 2525:25 rnwood/smtp4dev\n</code></pre></p> <p>After a few seconds, you can then open localhost:5000.</p> <p>If you're using a different Kubernetes runtime (e.g. k3d, microk8s, etc) you may need to set <code>smtp_host</code> to something other than <code>host.docker.internal</code> in your <code>chart/local.yaml</code>, and you may also need to configure other options. k3d likely uses <code>host.k3d.internal</code>, and microk8s <code>10.0.1.1</code>, but double check with your Kubernetes runtime documentation.</p>"},{"location":"develop/frontend-dev/","title":"Developing with Yarn","text":"<p>This guide explains how to build and serve the Browsertrix user interface with Yarn.</p> <p>Developing the user interface with Yarn bypasses the need to rebuild the entire frontend Docker image to view your UI changes. This setup is ideal for rapid UI development that does not rely on any backend changes.</p>"},{"location":"develop/frontend-dev/#requirements","title":"Requirements","text":""},{"location":"develop/frontend-dev/#1-browsertrix-api-backend-already-in-a-kubernetes-cluster","title":"1. Browsertrix API backend already in a Kubernetes cluster","text":"<p>The frontend development server requires an existing backend that has been deployed locally or is in production. See Deploying Browsertrix.</p> <p>Once deployed, make note of the URL to the backend API. If you've deployed the backend locally using default values, the URL will be <code>http://localhost:30870</code>.</p>"},{"location":"develop/frontend-dev/#2-nodejs-20","title":"2. Node.js \u226520","text":"<p>To check if you already have Node.js installed, run the following command in your command line terminal:</p> <pre><code>node --version\n</code></pre> <p>You should see a version number like <code>v20.17.0</code>. If you see a command line error instead of a version number, install Node.js before continuing.</p> What if my other project requires a different version of Node.js? <p>You can use Node Version Manager to install multiple Node.js versions and switch versions between projects.</p>"},{"location":"develop/frontend-dev/#3-yarn-1-classic","title":"3. Yarn 1 (Classic)","text":"<p>To verify your Yarn installation:</p> <pre><code>yarn --version\n</code></pre> <p>If your Yarn version starts with <code>1</code> (e.g. <code>1.22.22</code>) you're good to go.</p> <p>If Yarn isn't installed, install Yarn 1 (Classic).</p> <p>If your Yarn version is <code>2.0</code> or greater, run the following from your Browsertrix project directory to enable Yarn 1:</p> <pre><code>cd frontend\ncorepack enable\ncorepack install\n</code></pre> <p>Check out the full Yarn + Corepack installation guide for more details.</p>"},{"location":"develop/frontend-dev/#quickstart","title":"Quickstart","text":"<p>From the command line, change your current working directory to <code>/frontend</code>:</p> <pre><code>cd frontend\n</code></pre> <p>Note</p> <p>From this point on, all commands in this guide should be run from the <code>frontend</code> directory.</p> <p>Install UI dependencies:</p> <pre><code>yarn install\n</code></pre> <p>Copy environment variables from the sample file:</p> <pre><code>cp sample.env.local .env.local\n</code></pre> <p>Update <code>API_BASE_URL</code> in <code>.env.local</code> to point to your backend API URL noted earlier. For example, if connecting to your local deployment cluster:</p> <pre><code>API_BASE_URL=http://localhost:30870\n</code></pre> Port when using Minikube (on macOS) <p>When using Minikube on macOS, the port will not be 30870. Instead, Minikube opens a tunnel to a random port, obtained by running <code>minikube service browsertrix-cloud-frontend --url</code> in a separate terminal.</p> <p>Set API_BASE_URL to provided URL instead, eg. <code>API_BASE_URL=http://127.0.0.1:&lt;TUNNEL_PORT&gt;</code></p> <p>Note</p> <p>This setup assumes that your API endpoints are available under <code>/api</code>, which is the default configuration for the Browsertrix backend.</p> <p>Start the frontend development server:</p> <pre><code>yarn start\n</code></pre> <p>This will open <code>localhost:9870</code> in a new tab in your default browser.</p> <p>Saving changes to files in <code>src</code> will automatically reload your browser window with the latest UI updates.</p> <p>To stop the development server type Ctrl+C into your command line terminal.</p>"},{"location":"develop/frontend-dev/#scripts","title":"Scripts","text":"<code>yarn &lt;name&gt;</code> <code>start</code> runs app in development server, reloading on file changes <code>test</code> runs tests in chromium with playwright <code>build-dev</code> bundles app and outputs it in <code>dist</code> directory <code>build</code> bundles app, optimized for production, and outputs it to <code>dist</code> <code>lint</code> find and fix auto-fixable javascript errors <code>format</code> formats js, html, and css files <code>localize:extract</code> generate XLIFF file to be translated <code>localize:build</code> output a localized version of strings/templates"},{"location":"develop/frontend-dev/#testing","title":"Testing","text":"<p>Tests assertions are written in Chai.</p> <p>To watch for file changes while running tests:</p> <pre><code>yarn test --watch\n</code></pre> <p>To run tests in multiple browsers:</p> <pre><code>yarn test --browsers chromium firefox webkit\n</code></pre>"},{"location":"develop/frontend-dev/#logging","title":"Logging","text":"<p>Calls to <code>console.log()</code> and <code>console.debug()</code> are discarded by default in production, as configured in <code>frontend/webpack.prod.js</code>.</p>"},{"location":"develop/local-dev-setup/","title":"Setup for Local Development","text":""},{"location":"develop/local-dev-setup/#installation","title":"Installation","text":"<p>First, see our Local Deployment guide for instructions on how to install the latest release with Kubernetes with Helm 3.</p>"},{"location":"develop/local-dev-setup/#local-dev-configuration","title":"Local Dev Configuration","text":"<p>The local deployment guide explains how to deploy Browsertrix with latest published images.</p> <p>However, if you are developing locally, you will need to use your local images instead.</p> <p>We recommend the following setup:</p> <ol> <li> <p>Copy the provided <code>./chart/examples/local-config.yaml</code> Helm configuration file to a separate file <code>local.yaml</code>, so that local changes to it will not be accidentally committed to git.</p> <p>From the root directory:</p> <pre><code>cp ./chart/examples/local-config.yaml ./chart/local.yaml\n</code></pre> </li> <li> <p>Uncomment <code>backend_image</code>, <code>frontend_image</code>, and pull policies in <code>./chart/local.yaml</code>, which will ensure the local images are used: <pre><code>backend_image: docker.io/webrecorder/browsertrix-backend:latest\nemails_image: docker.io/webrecorder/browsertrix-emails:latest\nfrontend_image: docker.io/webrecorder/browsertrix-frontend:latest\nbackend_pull_policy: 'Never'\nemails_pull_policy: 'Never'\nfrontend_pull_policy: 'Never'\n</code></pre></p> MicroK8S <p>For microk8s, the pull policies actually need to be set to <code>IfNotPresent</code> instead of <code>Never</code>:</p> <pre><code>backend_pull_policy: 'IfNotPresent'\nemails_pull_policy: 'IfNotPresent'\nfrontend_pull_policy: 'IfNotPresent'\n</code></pre> <p>This will ensure images are pulled from the MicroK8S registry (configured in next section).</p> </li> <li> <p>Build the local backend and frontend images. The exact process depends on the Kubernetes environment you've selected in your initial deployment. Environment specific build instructions are as follows:</p> Docker Desktop <p>Rebuild the local images by running <code>./scripts/build-backend.sh</code> and/or <code>./scripts/build-frontend.sh</code> scripts to build the images in the local Docker.</p> MicroK8S <p>MicroK8s uses its own container registry, running on port 32000.</p> <ol> <li> <p>Ensure the registry add-on is enabled by running <code>microk8s enable registry</code></p> </li> <li> <p>Set <code>export REGISTRY=localhost:32000/</code> and then run <code>./scripts/build-backend.sh</code> and/or <code>./scripts/build-frontend.sh</code> to rebuild the images into the MicroK8S registry.</p> </li> <li> <p>In <code>./chart/local.yaml</code>, also uncomment the following lines to use the local images: <pre><code>backend_image: \"localhost:32000/webrecorder/browsertrix-backend:latest\"\nemails_image: \"localhost:32000/webrecorder/browsertrix-emails:latest\"\nfrontend_image: \"localhost:32000/webrecorder/browsertrix-frontend:latest\"\n</code></pre></p> </li> </ol> Minikube <p>Minikube comes with its own image builder to update the images used in Minikube.</p> <p>To build the backend image, run:</p> <pre><code>minikube image build -t webrecorder/browsertrix-backend:latest ./backend\n</code></pre> <p>To build the emails image, run:</p> <pre><code>minikube image build -t webrecorder/browsertrix-emails:latest ./emails\n</code></pre> <p>To build a local frontend image, run:</p> <pre><code>minikube image build -t webrecorder/browsertrix-frontend:latest ./frontend\n</code></pre> K3S <p>K3S uses <code>containerd</code> by default. To use local images, they need to be imported after rebuilding.</p> <ol> <li> <p>Rebuild the images with Docker by running by running <code>./scripts/build-backend.sh</code> and/or <code>./scripts/build-frontend.sh</code> scripts. (Requires Docker to be installed as well).</p> </li> <li> <p>Serializer the images to .tar: <pre><code>docker save webrecorder/browsertrix-backend:latest &gt; ./backend.tar\ndocker save webrecorder/browsertrix-emails:latest &gt; ./emails.tar\ndocker save webrecorder/browsertrix-frontend:latest &gt; ./frontend.tar\n</code></pre></p> </li> <li> <p>Import images into k3s containerd: <pre><code>k3s ctr images import --base-name webrecorder/browsertrix-backend:latest ./backend.tar\nk3s ctr images import --base-name webrecorder/browsertrix-emails:latest ./emails.tar\nk3s ctr images import --base-name webrecorder/browsertrix-frontend:latest ./frontend.tar\n</code></pre></p> </li> </ol> </li> <li> <p>To change other options, uncomment them as needed in <code>./chart/local.yaml</code> or add additional overrides from <code>./chart/values.yaml</code>.</p> <p>For example, to set a superuser email to <code>my_super_user_email@example.com</code> and password to <code>MySecretPassword!</code>, uncomment that block and set: <pre><code>superuser:\n# set this to enable a superuser admin\nemail: my_super_user_email@example.com\n\n# optional: if not set, automatically generated\n# change or remove this\npassword: MySecretPassword!\n</code></pre></p> </li> <li> <p>Once the images have been built and config changes made in <code>./chart/local.yaml</code>, the cluster can be re-deployed by running: <pre><code>helm upgrade --install -f ./chart/values.yaml \\\n-f ./chart/local.yaml btrix ./chart/\n</code></pre></p> MicroK8S <p>If using microk8s, the commend will be:</p> <pre><code>microk8s helm3 upgrade --install -f ./chart/values.yaml -f ./chart/local.yaml btrix ./chart/\n</code></pre> </li> </ol> <p>Refer back to the Local Development guide for additional information on running and debugging your local cluster.</p>"},{"location":"develop/local-dev-setup/#update-the-images","title":"Update the Images","text":"<p>After making any changes to backend code (in <code>./backend</code>) or frontend code (in <code>./frontend</code>), you'll need to rebuild the images as specified above, before running <code>helm upgrade ...</code> to re-deploy.</p> <p>Changes to settings in <code>./chart/local.yaml</code> can be deployed with <code>helm upgrade ...</code> directly.</p> Alternative method for developing the frontend <p>If you are not writing backend code or otherwise making changes to the backend, you can run the frontend outside of Docker to quickly iterate on the user interface. See UI Development for instructions on how to develop the frontend using Node.js tools.</p>"},{"location":"develop/localization/","title":"Localization","text":"<p>The Browsertrix UI supports multiple languages. Browsertrix end users can set a language preference in their account settings.</p>"},{"location":"develop/localization/#contributing","title":"Contributing","text":"<p>Translations are managed through Weblate, a web-based translation tool. Registration is free! Once registered, you can submit translations for review by Browsertrix maintainers.</p> <p>Register for Weblate</p>"},{"location":"develop/localization/#adding-a-language","title":"Adding a Language","text":"<p>Adding support for a new language involves a small code change. If you'd like to add a new language and would prefer that a Browsertrix maintainer make the change, submit a Localization Request on GitHub. A Browsertrix maintainer will respond to your request on GitHub.</p> <p>To add a new language directly through code change:</p> <ol> <li>Look up the BCP 47 language tag and add it to the <code>targetLocales</code> field in <code>lit-localize.json</code>.</li> </ol> <pre><code>{\n  // ...\n  \"sourceLocale\": \"en\",\n  \"targetLocales\": [\n    \"es\",\n    // Add your language tag here\n    ],\n}\n</code></pre> <ol> <li>Generate a new XLIFF file by running:</li> </ol> <pre><code>yarn localize:extract\n</code></pre> <p>This will add an <code>.xlf</code> file named after the new language tag to the <code>/xliff</code> directory.</p> <ol> <li>Open a pull request with the changes.</li> <li>Once the pull request is merged, manually refresh the language list in the Weblate Browsertrix project. Translations are managed entirely through the Weblate interface.</li> </ol> <p>New languages will be available in user preferences only after the app is redeployed.</p>"},{"location":"develop/localization/#making-strings-localizable","title":"Making Strings Localizable","text":"<p>All text should be wrapped in the <code>msg</code> helper to make them localizable:</p> <pre><code>import { msg } from \"@lit/localize\";\n\n// later, in the render function:\nrender() {\n  return html`\n    &lt;button&gt;\n      ${msg(\"Click me\")}\n    &lt;/button&gt;\n  `\n}\n</code></pre>"},{"location":"develop/localization/#handling-expressions-in-strings","title":"Handling expressions in strings","text":"<p>Expressions can be included in strings:</p> <pre><code>import { msg, str } from \"@lit/localize\";\n\nmsg(str`Welcome, ${name}.`)\n</code></pre> <p>Translators will see the string expression as-written in code. To aid translations, avoid calculations in expressions and choose a descriptive variable name.</p> <pre><code>// Instead of this:\n//\n// msg(str`This file exceeds the maximum of ${5 * 1000 * 1000} bytes.`).\n\n// Try this:\nconst bytes = 5 * 1000 * 1000;\n\nmsg(str`This file exceeds the maximum of ${bytes} bytes.`).\n</code></pre> <p>Dates and numbers should be localized and pluralized in source code before being assigned to the message string.</p> <p>For example:</p> <pre><code>import localize from \"@/utils/localize\";\nimport { pluralOf } from \"@/utils/pluralize\";\n\nconst date = localize.date(new Date());\nconst count = 200;\nconst number_of_URLs = `${localize.number(count)} ${pluralOf(\"URLs\", count)}`;\n\nmsg(str`You have ${number_of_URLs} pending as of ${date}.`);\n</code></pre> <p>Tip: Include a message description for translators.</p> <p>You can add additional context for translators using the <code>desc</code> option when the variable name may be ambiguous by itself.</p> <p>Building on the previous example:</p> <pre><code>msg(str`You have ${number_of_URLs} pending as of ${date}.`, {\n  desc: \"`number_of_URLs` example: '1,000 URLs'\"\n});\n</code></pre>"},{"location":"develop/localization/#handling-html-in-strings","title":"Handling HTML in strings","text":"<p>Lit supports HTML in translation strings. However, try to avoid including markup in strings by using multiple <code>msg()</code>s. In addition to a performance overhead, strings with HTML are more difficult to manage through the Weblate interface.</p> <pre><code>// Instead of this:\n//\n// msg(html`Would you like to continue? &lt;button&gt;Continue&lt;/button&gt;`)\n\n// Do this:\nhtml`\n  ${msg(\"Would you like to continue?\")} &lt;button&gt;${msg(\"Continue\")}&lt;/button&gt;\n`\n</code></pre> <p>When markup is unavoidable, prefer assigning the template to a variable.</p> <pre><code>const log_in = html`&lt;a href=\"/log-in\"&gt;${msg(\"log in\")}&lt;/a&gt;`\n\nmsg(html`Please ${log_in} to access this page.`, {\n  desc: \"`log_in` is a link to the log in page\"\n})\n</code></pre>"},{"location":"develop/ui/components/","title":"Writing Components","text":"<p>The Browsertrix UI is composed of web components written in TypeScript.</p> <p>Primary UI tools are:</p> <ul> <li>Lit \u2014 TypeScript framework for building reactive web components. All web application logic and templating is handled in Lit-enhanced web components.</li> <li>Shoelace \u2014 Web component library of common UI components like buttons and form elements. These components are themed with custom CSS variables.</li> <li>Tailwind \u2014 CSS utility library of composable classes. Browsertrix components are primarily styled by including Tailwind utility classes in HTML markup, which is rendered by Lit's template system.</li> <li><code>@open-wc/testing</code> \u2014 Test helpers for writing web component unit tests.</li> </ul>"},{"location":"develop/ui/components/#create-a-new-component","title":"Create a New Component","text":""},{"location":"develop/ui/components/#directory-structure","title":"Directory Structure","text":"<p>Component files should be created under <code>frontend/src</code> in the relevant folder:</p> <ul> <li><code>/components</code> \u2014 Common UI elements that can be reused throughout the web app.</li> <li><code>/features</code> \u2014 Specialized UI components that can be reused within a particular Browsertrix feature, but aren't generic enough to be reused across the web app. These components usually rely on specific data from the Browsertrix API.</li> <li><code>/pages</code> \u2014 Web components that correspond to a route.</li> </ul>"},{"location":"develop/ui/components/#naming-convention","title":"Naming Convention","text":"<p>Web components names are written are in kebab case (ex: <code>my-custom-component</code>). The component file is named after the component (ex: <code>my-custom-component.ts</code>).</p> <p>When defining a custom web component in Browsertrix, the <code>btrix-</code> prefix is added to the tag to distinguish Browsertrix components from third-party web components. Using the <code>my-custom-component</code> example, the component would appear in markup as such:</p> <pre><code>&lt;btrix-my-custom-component&gt;&lt;/btrix-my-custom-component&gt;\n</code></pre>"},{"location":"develop/ui/components/#defining-a-custom-component","title":"Defining a Custom Component","text":"<p>Browsertrix includes extensible TypeScript classes for defining custom components. One of the following classes should be used to define a web component, rather than extending <code>LitElement</code> directly.</p> <ul> <li><code>TailwindElement</code> \u2014 Components that are styled with Tailwind CSS utility classes. Use to define simple, styled UI elements that do not need access to global UI state, do not make API calls, and do not have any global side effects (like navigation.)</li> <li><code>BtrixElement</code> \u2014 Styled components that are contextualized with shared UI state. Use to define complex components that need access to global state to make API calls, and produce global side effects like navigation, toast alerts, or user locale changes.</li> </ul> <p>Regardless of the base class, Browsertrix components are composable. A common pattern is to create a <code>BtrixElement</code> that composes multiple <code>TailwindElement</code> components or Shoelace components.</p> <p>The following example is of a component that extends <code>BtrixElement</code> to access the current user's name in global state (<code>this.appState.userInfo</code>) and renders it as a custom confirmation composed of multiple <code>TailwindElement</code> components (<code>&lt;btrix-alert&gt;</code>, <code>&lt;btrix-button&gt;</code>).</p> <pre><code>// my-custom-component.ts\n@customElement(\"btrix-my-custom-component\")\nclass MyCustomComponent extends BtrixElement {\n  render() {\n    return html`\n      &lt;btrix-alert&gt;\n        Hello, are you ${this.appState.userInfo.name}?\n\n        &lt;btrix-button&gt;Yes&lt;/btrix-button&gt;\n      &lt;/btrix-alert&gt;\n    `;\n  }\n}\n</code></pre>"},{"location":"develop/ui/components/#vs-code-snippet","title":"VS Code Snippet","text":"<p>If developing with Visual Studio Code, you can generate boilerplate for a <code>BtrixElement</code> Browsertrix component by typing in <code>component</code> to any TypeScript file and selecting \"Btrix Component\". Hit Tab to move your cursor between fillable fields in the boilerplate code.</p>"},{"location":"develop/ui/components/#unit-testing","title":"Unit Testing","text":"<p>Unit test files live next to the component file and are suffixed with <code>.test</code> (ex: <code>my-custom-component.test.ts</code>).</p> <p>You can also generate boilerplate for a component test in VS Code by creating a new <code>.test.ts</code> file, then typing <code>test</code> and selecting \"Btrix Component Test\".</p>"},{"location":"develop/ui/storybook/","title":"Using Storybook","text":"<p>Storybook is a tool for documenting and building UI components and design systems in isolation. Component documentation is organized into \"stories\" that show a variety of possible rendered states of a UI component.</p> <p>Browsertrix component stories live in <code>frontend/src/stories</code>. Component attributes that are public properties (i.e. defined with Lit <code>@property({ type: Type })</code>) or documented in a TSDoc comment will automatically appear in stories through the Custom Elements Manifest file.</p> <p>To develop using Storybook, run:</p> <pre><code>yarn storybook:watch\n</code></pre> <p>This will open Storybook in your default browser. Changes to Browsertrix components and stories wil automatically refresh the page.</p>"},{"location":"user-guide/","title":"Introduction","text":"<p>Browsertrix is an intuitive, automated web archiving platform designed to allow you to archive, replay, and share websites exactly as they were at a certain point in time.</p> <p>Browsertrix is built and hosted by Webrecorder, a leading expert in web archiving. Our goal is to make web archiving easier and more accessible to everyone through open source tools, easy-to-use interfaces, and community building.</p> <p>This user guide documents features, terminology, and settings in the Browsertrix web interface. The user guide navigation is organized similarly to the Browsertrix web interface for ease of browsing. You can also use the search box up top to search by topic.</p> <p>If you have any feedback on our documentation we'd love to hear it. We've written the user guide with both seasoned web archivists and new archivists in mind and we value feedback from all levels of archiving experience.</p> <p>For help with a specific topic, try our community help forum.</p> <p>For bug reports or feature requests, please open a GitHub issue.</p> <p>Thanks for choosing Browsertrix! Ready for your first crawl?</p>"},{"location":"user-guide/archived-items/","title":"Intro to Archived Items","text":"<p>Archived items consist of one or more WACZ files created by a crawl workflow or uploaded to Browsertrix. They can be individually replayed, or combined with other archived items in a collection. The Archived Items page lists all items in the organization.</p>"},{"location":"user-guide/archived-items/#uploading-web-archives","title":"Uploading Web Archives","text":"<p>WACZ files can be given metadata and uploaded to Browsertrix by pressing the Upload WACZ button on the archived items list page. Only one WACZ file can be uploaded at a time.</p>"},{"location":"user-guide/archived-items/#status","title":"Status","text":"<p>The status of an archived item depends on its type. Uploads will always have the status   Uploaded, crawls have four possible states:</p> Status Description  Complete The crawl completed according to the workflow's settings. Workflows with crawl limits set may stop running before they capture every queued page, but the resulting archived item will still be marked as \"Complete\".  Stopped The crawl workflow was stopped gracefully by a user and data is saved.  Stopped: Reason A workflow limit (listed as the reason) was reached and data is saved.  Canceled The crawl workflow was canceled by a user, no data is saved.  Failed A serious error occurred while crawling, no data is saved. <p>Because   Canceled and   Failed crawls do not contain data, they are omitted from the archived items list page and cannot be added to a collection.</p>"},{"location":"user-guide/archived-items/#archived-item-details","title":"Archived Item Details","text":"<p>The archived item details page is composed of the following sections, though some are only available for crawls and not uploads.</p>"},{"location":"user-guide/archived-items/#overview","title":"Overview","text":"<p>View metadata and statistics associated with how the archived item was created.</p> <p>Metadata can be edited by pressing the pencil icon at the top right of the metadata section to edit the item's description, tags, and collections it is associated with.</p>"},{"location":"user-guide/archived-items/#quality-assurance","title":"Quality Assurance","text":"<p>View crawl quality information collected from analysis runs, review crawled pages, and start new analysis runs. QA is only available for crawls and org members with crawler permissions.</p> <p>The pages list provides a record of all pages within the archived item, as well as any ratings or notes given to the page during review. If analysis has been run, clicking on a page in the pages list will go to that page in the review interface.</p>"},{"location":"user-guide/archived-items/#crawl-analysis","title":"Crawl Analysis","text":"<p>Running crawl analysis will re-visit all pages within the archived item, comparing the data collected during analysis with the data collected during crawling. Crawl analysis runs with the same workflow limit settings used during crawling.</p> <p>Crawl analysis can be run multiple times, though results should only differ if the crawler version has been updated between runs. The analysis process is being constantly improved and future analysis runs should produce better results. Analysis run data can be downloaded or deleted from the Analysis Runs tab. While they are stored as WACZ files, analysis run WACZs only contain analysis data and may not open correctly or be useful in other programs that replay archived content.</p> <p>Once a crawl has been analyzed \u2014 either fully, or partially \u2014 it can be reviewed by pressing the Review Crawl button. For more on reviewing crawls and how to interpret analysis data, see: Crawl Review.</p> <p><code>Paid Feature</code></p> <p>Like running a crawl workflow, running crawl analysis also uses execution time. Crawls and crawl analysis share the same concurrent crawling limit, but crawl analysis runs will be paused in favor of new crawls if the concurrent crawling limit is reached.</p>"},{"location":"user-guide/archived-items/#replay","title":"Replay","text":"<p>View a high-fidelity replay of the website at the time it was archived.</p> <p>For more details on navigating web archives within ReplayWeb.page, see the ReplayWeb.page user documentation.</p>"},{"location":"user-guide/archived-items/#exporting-files","title":"Exporting Files","text":"<p>While crawling, Browsertrix will output one or more WACZ files \u2014 the crawler aims to output files in consistently sized chunks, and each crawler will output separate WACZ files.</p> <p>The WACZ Files tab lists the individually downloadable WACZ files that make up the archived item as well as their file sizes and backup status.</p> <p>To download an entire archived item as a single WACZ file, click the Download Item button at the top of the WACZ Files tab or the Download Item entry in the crawl's Actions menu.</p> <p>To combine multiple archived items and download them all as a single WACZ file, add them to a collection and download the collection.</p>"},{"location":"user-guide/archived-items/#logs","title":"Logs","text":"<p>View a list of errors and behavior logs that were generated during crawling. Clicking a log entry in the list will reveal additional information.</p> <p>Only a subset of the logs generated by the crawler are visible in this tab. All log entries that were recorded in the creation of the archived item can be downloaded in JSONL format by pressing the Download All Logs button.</p>"},{"location":"user-guide/archived-items/#crawl-settings","title":"Crawl Settings","text":"<p>View the crawl workflow configuration options that were used to generate the resulting archived item. Many of these settings also apply when running crawl analysis.</p>"},{"location":"user-guide/collection/","title":"Intro to Collections","text":"<p>A collection is a specific, user-directed grouping of archived items from either crawls or WACZ files. You can create a collection, add content to your collection, include a description to your collection, download your collection, and share your collection whichever way you need to others in your community.</p>"},{"location":"user-guide/collection/#create-a-collection","title":"Create a Collection","text":"<p>You can create a collection from the Collections page directly, or the  Create New... shortcut from the org dashboard.</p>"},{"location":"user-guide/collection/#add-collection-content","title":"Add Collection Content","text":"<p>Collections are the primary way of organizing and combining archived items into groups for presentation. Collections also allow you to view a combined replay of any archived items they contain; if a link is present when viewing a collection but the actual page is missing, and another item with that captured page is added to the collection, the link will now work as expected.</p> <p>Tip: Patching a crawl with interactive archiving</p> <p>If the crawler has not captured every resource or interaction on a webpage, our ArchiveWeb.page browser extension can be used to interactively capture missing content using your web browser and upload it directly to your org.</p> <p>After adding crawls and uploads to a collection, content from both will become available in the replay viewer.</p> <p>Crawls and uploads can be added to a collection after creation by selecting Select Archived Items from the collection's actions menu.</p> <p>A crawl workflow can also be set to automatically add any completed crawls to a collection in the workflow's settings.</p>"},{"location":"user-guide/collection/#customize-a-collection","title":"Customize a Collection","text":"<p>The Presentation and Sharing page provides further details for options on how to present and share Collections. You can customize how your Collection appears to the public by clicking the edit button   in each collection to:</p> <ul> <li> <p>Name it and add a description \u2014 include emojis if that\u2019s your style \ud83d\ude09</p> </li> <li> <p>Select a thumbnail to represent it: Choose between a page screenshot and a placeholder</p> </li> <li> <p>Choose the initial view for your Collection</p> </li> </ul>"},{"location":"user-guide/collection/#download-a-collection","title":"Download a Collection","text":"<p>Downloading a collection will export every archived item in it as a single WACZ file. To download a collection, use the Download Collection option under the collection's Actions dropdown.</p>"},{"location":"user-guide/collection/#collection-access","title":"Collection Access","text":"<p>Collections can be set to one of the one following access modes.</p> <ul> <li>Private \u2014 Collections only accessible to logged in users</li> <li>Unlisted \u2014 Collections can be shared with others, given the link to the collection.</li> <li>Public \u2014 Collections can be shared with others, and when enabled, collections can also be listed in the public collections gallery.</li> </ul> <p>Note: About Public Collections Gallery</p> <p>If the public collections gallery page is not enabled, any existing public collections are treated the same as unlisted collections. Check out enabling public collection gallery for a guide on how to enable this page.</p>"},{"location":"user-guide/contribute/","title":"Contribute","text":"<p>We hope our user guide is a useful tool for you. Like Browsertrix itself, our user guide is open source. We greatly appreciate any feedback and open source contributions to our docs on GitHub.</p> <p>Other ways to contribute:</p> <ol> <li>Answer questions from the web archiving community on the community help forum.</li> <li>Let us know how we can improve our documentation.</li> <li>If you encounter any bugs while using Browsertrix, please open a GitHub issue or contact support.</li> </ol>"},{"location":"user-guide/crawl-workflows/","title":"Intro to Crawl Workflows","text":"<p>Crawl workflows are the bread and butter of automated browser-based crawling. A crawl workflow enables you to specify how and what the crawler should capture on a website.</p> <p>A finished crawl results in an archived item that can be downloaded and shared. To easily identify and find archived items within your org, you can automatically name and tag archived items through custom workflow metadata.</p> <p>You can create, view, search for, and run crawl workflows from the Crawling page.</p>"},{"location":"user-guide/crawl-workflows/#create-a-crawl-workflow","title":"Create a Crawl Workflow","text":"<p>Create new crawl workflows from the Crawling page, or the  Create New ... shortcut from Dashboard.</p>"},{"location":"user-guide/crawl-workflows/#choose-what-to-crawl","title":"Choose what to crawl","text":"<p>The first step in creating a new crawl workflow is to choose what you'd like to crawl by defining a Crawl Scope. Crawl scopes are categorized as a Page Crawl or Site Crawl.</p>"},{"location":"user-guide/crawl-workflows/#page-crawl","title":"Page Crawl","text":"<p>Choose one of these crawl scopes if you know the URL of every page you'd like to crawl and don't need to include any additional pages beyond one hop out.</p> <p>A Page Crawl workflow is simpler to configure, since you don't need to worry about configuring the workflow to exclude parts of the website that you may not want to archive.</p>"},{"location":"user-guide/crawl-workflows/#site-crawl","title":"Site Crawl","text":"<p>Choose one of these crawl scopes to have the the crawler automatically find pages based on a domain name, start page URL, or directory on a website.</p> <p>Site Crawl workflows are great for advanced use cases where you don't need (or want) to know every single URL of the website that you're archiving.</p> <p>After deciding what type of crawl you'd like to run, you can begin to set up your workflow. A detailed breakdown of available settings can be found in the workflow settings guide.</p>"},{"location":"user-guide/crawl-workflows/#run-crawl","title":"Run Crawl","text":"<p>Run a crawl workflow by clicking Run Crawl in the actions menu of the workflow in the crawl workflow list, or by clicking the Run Crawl button on the workflow's details page.</p> <p>While crawling, the Latest Crawl section streams the current state of the browser windows as they visit pages. You can modify the crawl live by adding URL exclusions or changing the number of crawling instances.</p> <p>Re-running a crawl workflow can be useful to capture a website as it changes over time, or to run with an updated crawl scope.</p>"},{"location":"user-guide/crawl-workflows/#status","title":"Status","text":"<p>Finished crawl workflows inherit the status of the last archived item they created. Crawl workflows that are in progress maintain their own statuses.</p>"},{"location":"user-guide/getting-started/","title":"Your First Crawl","text":"<p>Let\u2019s crawl your first webpage! Start by opening up a webpage that you'd like to crawl, and note the URL for later.</p>"},{"location":"user-guide/getting-started/#accessing-your-dashboard","title":"Accessing your dashboard","text":"<p>To start a crawl, you'll need to log in using a Browsertrix account that has crawler permissions.</p> <p>You likely have crawler permissions already if:</p> <ul> <li>You registered for an org on hosted Browsertrix</li> <li>You joined an existing org and were given crawler permissions</li> <li>You are the admin of a self-hosted instance</li> </ul> <p>Check if you have crawler permissions by logging in. If you see a + Create New... button near the org name, you're able to start a crawl. If you don't see this button and think that you should, contact your org admin to update your permissions.</p>"},{"location":"user-guide/getting-started/#starting-the-crawl","title":"Starting the crawl","text":"<p>When you log in, the first page you see is the org dashboard. If you've navigated away to another page, navigate back to Dashboard.</p> <ol> <li>Tap the Create New... shortcut and select Crawl Workflow.</li> <li>Enter the URL of the webpage that you noted earlier as the URL to Crawl.</li> <li>Tap Run Crawl.</li> <li>You should now see your new crawl workflow running. Give the crawler a few moments to warm up, and then watch as it archives the webpage!</li> </ol>"},{"location":"user-guide/getting-started/#next-steps","title":"Next steps","text":"<p>After running your first crawl, check out the following to learn more about Browsertrix's features:</p> <ul> <li>A detailed list of crawl workflow setup options.</li> <li>Adding exclusions to limit your crawl's scope and evading crawler traps by editing exclusion rules while crawling.</li> <li>Best practices for crawling with browser profiles to capture content only available when logged in to a website.</li> <li>Managing archived items, including uploading previously archived content.</li> <li>Organizing and combining archived items with collections for sharing and export.</li> <li>Invite collaborators to your org.</li> </ul>"},{"location":"user-guide/join/","title":"Join an Existing Org","text":"<p>If you've received an email inviting you to an org, you can join the org by accepting the invitation.</p> <p>If you don't have a Browsertrix account, you'll be prompted to create a password and display name.</p>"},{"location":"user-guide/org-members/","title":"Manage Org Members","text":"<p>View and manage all current members who have access to the organization, as well as any invited members who have not yet accepted an invitation to join the organization. In the Active Members table, you can change the permission level of all users in the organization, including other admins. An organization must have at least one admin. Admins can also remove members by pressing the trash button.</p> <p>You can add new members to the organization by pressing the Invite New Member button. Enter the email address associated with the user, select the appropriate role, and press Invite to send a link to join the organization via email.</p> <p>Sent invites can be invalidated by pressing the trash button in the relevant Pending Invites table row.</p>"},{"location":"user-guide/org-members/#permission-levels","title":"Permission Levels","text":"<code>Viewer</code> Users with the viewer role have read-only access to all material within the organization. They cannot create or edit archived items, crawl workflows, browser profiles, or collections. They also do not have access to any crawl analysis or review tools. <code>Crawler</code> Users with the crawler role can create crawl workflows and collections, but they cannot delete existing archived items that they were not responsible for creating. <code>Admin</code> Users with the administrator role have full access to the organization, including its settings page."},{"location":"user-guide/org-settings/","title":"Edit Org Settings","text":"<p>Settings that apply to the entire organization are found in the Settings page. If you're an org admin, you'll see the link to Settings in the org navigation bar.</p>"},{"location":"user-guide/org-settings/#general","title":"General","text":""},{"location":"user-guide/org-settings/#name-url-and-other-basic-information","title":"Name, URL, and other basic information","text":"<p>Your org name appears throughout the web application and in email notifications. Choose a display name for your org that's unique and memorable, like the name of your company, organization, or personal project.</p> <p>The org URL is where you and other org members will go to view the dashboard, configure org settings, and manage all other org-related activities. If your org has a public collections gallery enabled, changing this will also update the URL to the gallery.</p> <p>Org name and URLs are unique to each Browsertrix instance (for example, on <code>app.browsertrix.com</code>) and you may be prompted to change the org name or URL if either are already in use by another org.</p> What information will be visible to the public? <p>All org information is private until you make the org visible. Once your org is made visible to the public, the org name, description, and website will appear on the org's public collections gallery page. You can preview how information appears to the public by visiting the linked public collections gallery page.</p>"},{"location":"user-guide/org-settings/#public-collections-gallery","title":"Public Collections Gallery","text":"<p>Enable a homepage for your public collections to easily share all public collections in your org. Once enabled, anyone on the internet with a link to your org's public collections gallery will be able to browse public collections and view general information like the org name, description, and website.</p>"},{"location":"user-guide/org-settings/#billing","title":"Billing","text":"<p><code>Paid Feature</code></p> <p>View and manage your org's current payment plan and associated quotas. Usage history statistics for previous months are shown here to better inform your plan and quota requirements.</p>"},{"location":"user-guide/org-settings/#crawling-defaults","title":"Crawling Defaults","text":"<p>Set default suggested settings for all new crawl workflows. When creating a new workflow, org members will see the form pre-filled with these default values. Org members can still change or remove these settings when configuring the workflow. Removing a default setting will revert the setting back to Browsertrix' defaults.</p>"},{"location":"user-guide/org/","title":"Introduction to Orgs","text":"<p>A Browsertrix org, or organization, is your workspace for web archiving. If you\u2019re archiving collaboratively, an org workspace can be shared between team members.</p> <p>Every Browsertrix user belongs to one or more orgs. You'll create an org during sign-up if you sign up for Browsertrix through one of our hosted plans.</p> <p>Billing is managed per-org. Depending on your plan, your org may have monthly quotas for storage and minutes of crawl time. These quotas can be increased by upgrading your plan.</p> <p>You can change your org name, org URL, default workflow settings, and more from the Settings page.</p>"},{"location":"user-guide/overview/","title":"View Usage Stats and Quotas","text":"<p>Your Dashboard delivers key statistics about the org's resource usage. You can also create crawl workflows, upload archived items, create collections, and create browser profiles through the Create New ... shortcut.</p>"},{"location":"user-guide/overview/#storage","title":"Storage","text":"<p>The storage panel displays the total size and count of archived items and browser profiles.</p> <p>For organizations with a set storage quota, the storage panel displays a visual breakdown of how much space the organization has left and how much has been taken up by all types of archived items and browser profiles. To view additional information about each item, hover over its section in the graph.</p> Miscellaneous storage <p>You may see an additional Miscellaneous size depending on your crawl workflow and collection configuration. Miscellaneous is the total size of all supplementary files in use by your organization, such as workflow URL list files and custom collection thumbnails.</p>"},{"location":"user-guide/overview/#crawling","title":"Crawling","text":"<p>The crawling panel lists the number of currently running and waiting crawls, as well as the total number of pages captured.</p>"},{"location":"user-guide/overview/#execution-time","title":"Execution Time","text":"<p><code>Paid Feature</code></p> <p>For organizations with a set execution minute limit, the crawling panel displays a graph of how much execution time has been used and how much is currently remaining. Monthly execution time limits reset on the first of each month at 12:00 AM GMT.</p> How is execution time calculated? <p>Execution time is the total runtime of a crawl scaled by the Browser Windows value during a crawl. Like elapsed time, this is tracked while the crawl runs. Changing the amount of Browser Windows while a crawl is running may change the amount of execution time used in a given time period.</p>"},{"location":"user-guide/overview/#collections","title":"Collections","text":"<p>The collections panel displays the number of total collections and collections marked as sharable.</p>"},{"location":"user-guide/presentation-sharing/","title":"Presentation &amp; Sharing","text":""},{"location":"user-guide/presentation-sharing/#share-a-collection","title":"Share a Collection","text":"<p>A Collection is private by default, but can be made sharable either with an unlisted link, or displayed in your org's public gallery for everyone to discover. Sharing settings can be found within the Sharing tab of the Collection.</p> <ol> <li> <p>Click the edit button   on the specific Collection you want to share</p> </li> <li> <p>In that Collection's Sharing tab, change its visibility from the default Private to Public and save</p> </li> <li> <p>Click on the edit button   again, and now in the Collection's Sharing tab, there will be a new section, Link to Share, with a URL for this specific Collection that you can share and circulate to your community</p> </li> <li> <p>Optional: If you want to allow other viewers to download the Collection, confirm that the Show download button is enabled. If not, toggle it so that others would not be able to download it and only view.</p> </li> </ol>"},{"location":"user-guide/presentation-sharing/#customize-a-collection","title":"Customize a Collection","text":"<p>You can customize how your Collection appears to the public by making edits within the Presentation tab of Collection.</p> <p>You can access this tab via the edit button   of a Collection from the Public Collection Gallery or via the kebab icon of a Collection from your account's Collections tab.</p> <p>From the Collections tab:</p> <ol> <li> <p>Click the Collections tab from your account</p> </li> <li> <p>Click the kebab icon   for options</p> </li> <li> <p>Click Edit Collection Settings</p> </li> <li> <p>Edit any of the following information from the Presentation tab: Name, Summary, Thumbnail, and set Initial View</p> </li> </ol> <p>From the Public Collection Gallery. When Public Collection Gallery is enabled, you can view Collections from your dashboard. To customize the Collection:</p> <ol> <li> <p>Click on the edit button   of the Collection you want to edit</p> </li> <li> <p>Edit any of the following information from the Presentation tab: Name, Summary, Thumbnail, and set Initial View</p> </li> </ol>"},{"location":"user-guide/presentation-sharing/#name","title":"Name","text":"<p>A Name is always required, so by default, your Collection's name is determined by the name of your crawl workflow.</p> <p>You can edit the Name as long as its within 50 characters, which is roughly between 7 words and 13 words with spaces included in the character count.</p>"},{"location":"user-guide/presentation-sharing/#summary","title":"Summary","text":"<p>A Summary describing your Collection is not required, but it is useful information to others to summarize the Collection and caption collection thumbnails.</p> <p>You can edit the Summary as long as its within 150 characters, which is roughly between 90 words and 150 words with spaces included in the character count.</p>"},{"location":"user-guide/presentation-sharing/#thumbnail","title":"Thumbnail","text":"<p>Choose a thumbnail image to represent this collection in your org's dashboard and public collections gallery page. Thumbnails can either be a placeholder image designed by Webrecorder, or a screenshot of a page from the collection.</p> <p>To choose a screenshot:</p> <ol> <li> <p>Click the Enter a Page URL to preview it for a Page URL dropdown to appear</p> </li> <li> <p>Select one of the URLs or search for a specific URL from the Collection</p> </li> <li> <p>Optional: You can choose a specific captured URL based on its Page Timestamp.</p> </li> </ol> Why isn't there a thumbnail preview for my page? <p>If you have uploaded archived items captured with tools other than Browsertrix, some of the pages in your collection may not have screenshots available to use as thumbnails. To fix this, either re-crawl a page you wish to use as a thumbnail with Browsertrix and add it to the collection, or choose a placeholder thumbnail.</p>"},{"location":"user-guide/presentation-sharing/#initial-view","title":"Initial View","text":"<p>The Initial View is the first view visitors see first when viewing a Collection. The initial view can either be a list of pages, which is the default when replaying using ReplayWeb.page, or a single page from your collection such as a crawl start URL or index page.</p>"},{"location":"user-guide/public-collections-gallery/","title":"Public Collections Gallery","text":"<p>Collections provide a way to dynamically combine and group multiple individual crawls and uploads into a contextual, unified archive replay experience.</p> <p>With Public Collections Gallery, you can create a dedicated page that showcases all public Collections in one place, allowing you to personalize, curate, and share your web archives with the world!</p>"},{"location":"user-guide/public-collections-gallery/#enable-public-collections-gallery","title":"Enable Public Collections Gallery","text":"<p>The enable public collections gallery toggle is located in the org's Settings tab within your account. Follow these steps to activate the Public Collections Gallery.</p> <ol> <li> <p>Log in to your Browsertrix account</p> </li> <li> <p>In your org's settings, click enable public collections gallery and save</p> </li> <li> <p>You can share the link to your Public Collections Page from either copying from the org's settings, or by clicking on the copy button next to \"Visit public collections gallery\" from your dashboard.</p> </li> </ol> <p>Note: If you choose to hide the gallery later, you can go back to the setting and set the toggle to off at any time. The gallery will still be available for logged in users to preview, but the public page will be hidden. Public Collections will be treated the same as Unlisted when the public gallery is turned off.</p>"},{"location":"user-guide/public-collections-gallery/#customizing-the-gallery","title":"Customizing the Gallery","text":"<p>The Public Collection Gallery page will automatically have the title of your Org. You can customize this page by including a Description and your org's Website by adding them from your org's Settings of your account. The Gallery URL is always based on the Org URL, if you change one, the other changes as well.</p> <p>For example, if your org URL ends with <code>/my-org</code>, than the Public Collections Gallery will be made available at <code>/explore/my-org</code>.</p>"},{"location":"user-guide/public-collections-gallery/#add-collections","title":"Add Collections","text":"<p>If you don\u2019t have any public Collections yet, on the right side of the section you'll see three different icons.</p> <ol> <li> <p>Click the asterisk icon   in order to see all your Collections.</p> </li> <li> <p>Click the edit button   on the specific Collection you want to share</p> </li> <li> <p>In that Collection's Sharing tab, change its visibility from the default Private to Public and save</p> </li> <li> <p>Repeat Steps 2 and 3 as needed to share all the Collections you want</p> </li> <li> <p>When you're done, click the globe icon   in order to see all your public Collections</p> </li> <li> <p>Click on \u201cVisit Public Collections Gallery\u201d, and now you can share your gallery with the world!</p> </li> </ol>"},{"location":"user-guide/public-collections-gallery/#remove-collections","title":"Remove Collections","text":"<p>You'll take similar steps from Add Collections to also remove Collections from the Public Collections Gallery</p> <ol> <li> <p>Within the Public Collections Gallery, click the edit button   on the specific Collection you want to remove</p> </li> <li> <p>In that Collection's Sharing tab, change its visibility from the default Public to Private, save, and that Collection should be removed from the Public Collections Gallery!</p> </li> </ol>"},{"location":"user-guide/public-collections-gallery/#customize-collections","title":"Customize Collections","text":"<p>The Presentation and Sharing page provides further details for options on how to present and share Collections in your Public Collections Gallery. Your Public Collection Gallery page will automatically have the title of your org. You can customize this page by including a Description and your org's Website by adding them from your org's Settings of your account.</p>"},{"location":"user-guide/public-collections-gallery/#description","title":"Description","text":"<p>A Description about your Collection is not required, but it is useful information to share additional context and details of your collection with your org team members or the public by writing a description.</p> <p>You can edit the Description as long as its within 150 characters, which is roughly between 90 words and 150 words with spaces included in the character count. The Description supports basic text formatting like headings, bold and italicized text, lists, and links.</p>"},{"location":"user-guide/public-collections-gallery/#website","title":"Website","text":"<p>Link to your org's (or your personal) website so viewers can visit from your Public Collections Gallery page.</p> <p>how your Collection appears to the public by clicking the edit button   in each collection to:</p> <ul> <li> <p>Name it and add a description \u2014 include emojis if that\u2019s your style \ud83d\ude09</p> </li> <li> <p>Select a thumbnail to represent it: Choose between a page screenshot and a placeholder</p> </li> <li> <p>Choose the initial view for your collection.</p> </li> </ul>"},{"location":"user-guide/qa-review/","title":"Review Crawl","text":""},{"location":"user-guide/qa-review/#overview-of-crawl-quality","title":"Overview of Crawl Quality","text":"<p>In a QA analysis, Browsertrix collects data in two stages: first during the initial crawl, and then again during the replay. Rather than comparing the replay to the live site, we compare it to the data captured during the crawl. This ensures that the web archive that is to be downloaded, added to a collection, or shared provides a high quality replay.</p> <p>When reviewing the page, you will be able to analyze specific elements beginning with the Starting URL. </p> <p>You will be able to review the crawled pages by:</p> <ul> <li>Screenshots: A static visual snapshot of a section of the captured page</li> <li>Text: A full transcript of the text within the page</li> <li>Resources: Web documents (i.e. HTML, stylesheets, fonts, etc.) that make up the page</li> </ul> <p>Navigation Prevented in Replay within QA</p> <p>To navigate through the captured website, use the Replay feature in the Crawling section. Links will not be clickable when using the Replay tab within the Analysis view.</p> <p>Limited View in Default Mode</p> <p>When you first view an analysis of a page, the screenshot, text, and resource comparison views are only available for analyzed crawls. You'll need to run an analysis to view and compare all quality metrics.</p>"},{"location":"user-guide/qa-review/#qa-on-your-web-archive","title":"QA on Your Web Archive","text":"<p>When you run an analysis, you'll have a comparison view of the data collected. If multiple analysis runs have been completed, the page data will be used from the selected analysis run, which are displayed next to the archived item name. The most recent analysis run is selected by default, but you can choose to display data from any other completed or stopped analysis run here as well.</p> <p>The depth of your page review may vary depending on available time and the complexity of the page. For automated support, crawl analysis can generate comparisons across three key factors to help highlight potentially problematic pages. If you prefer a manual approach, you can still assess crawls even without running an analysis. You\u2019re still able to review page quality manually and leave comments, provide ratings, and examine the screenshots, text, and resources.</p>"},{"location":"user-guide/qa-review/#screenshot-comparison","title":"Screenshot Comparison","text":"<p>Screenshots are compared by measuring the perceived difference between color samples and by the intensity of difference between pixels. These metrics are provided by the open source tool Pixelmatch.</p> <p>Discrepancies between crawl and replay screenshots may occur because resources aren't loaded or rendered properly (usually indicating a replay issue).</p> <p>Caveats</p> <p>If many similar pages exhibit similarly poor screenshot comparison scores but look fine in the replay tab, it may be because of page loading time not being long enough during analysis.</p> <p>Some websites may take more time to load than others, including on replay! If the page wasn't given enough time to load during crawl analysis \u2014 because crawl analysis uses the same workflow limit settings as crawling \u2014 increasing the Delay After Page Load workflow setting may yield better screenshot analysis scores, at the cost of extra execution time.</p>"},{"location":"user-guide/qa-review/#extracted-text-comparison","title":"Extracted Text Comparison","text":"<p>Text extracted during crawl analysis is compared to the text extracted during crawling. Text is compared on the basis of Levenshtein distance.</p> <p>Resources not loaded properly on replay may display ReplayWeb.page's <code>Archived Page Not Found</code> error within the extracted text.</p>"},{"location":"user-guide/qa-review/#resource-comparison","title":"Resource Comparison","text":"<p>The resource comparison tab displays a table of resource types, and their HTTP status code count grouped by \"good\" and \"bad\". 2xx &amp; 3xx range status codes are assigned \"good\", 4xx &amp; 5xx range status codes are assigned \"bad\". Bad status codes on crawl indicate that a resource was not successfully captured. Bad status codes on replay that marked good when crawling usually indicate a replay issue.</p> <p>Caveats</p> <p>The number of resources may be higher on replay due to how components of ReplayWeb.page re-write certain request types. A discrepancy alone may not be an indicator that the page is broken, though generally it is a positive sign when the counts are equal.</p> <p>Due to the complicated nature of resource count comparison, this is not available as a sorting option in the pages list.</p>"},{"location":"user-guide/qa-review/#page-review","title":"Page Review","text":"<p>The pages from the crawl will be listed so you can click on pages based on particular interest, comparison rating, or just random spot checking from your workflow.</p> Should I review every page? <p>Probably not! When reviewing a crawl of a site that has many similar pages, all of which exhibit the same error and have similar heuristic scores, it's likely that they all are similarly broken, and you can probably save yourself the trouble. Depending on the website, the heuristic scores may not always be an accurate predictor of quality, but in our testing they are fairly consistent \u2014 consistency being the important factor of this tool. It is up to you, the curator, to make the final quality judgement!</p> <p>Our recommended workflow is as follows: run crawl analysis, examine the most severe issues as highlighted, examine some key examples of common layouts, review any other key pages, and score the crawl accordingly!</p>"},{"location":"user-guide/qa-review/#finish-review","title":"Finish Review","text":"<p>Once a satisfactory amount of pages have been reviewed, press the Finish Review button to give the archived item an overall quality score ranging from \"Excellent!\" to \"Bad\". You can add any additional notes or considerations in the archived item description, which can be edited during this step.</p>"},{"location":"user-guide/qa-review/#rate-this-crawl","title":"Rate This Crawl","text":"<p>This quality score helps others in your organization understand how well the page was captured and whether it needs to be recaptured. You can choose from the following rating ranges: - Excellent! This archived item perfectly replicates the original pages - Good. Looks and functions nearly the same as the original pages - Fair. Similar to the original pages, but may be missing non-critical content or functionality - Bad. Missing all content and functionality from original pages</p>"},{"location":"user-guide/qa-review/#update-crawl-metadata","title":"Update Crawl Metadata","text":"<p>You can include additional metadata in the provided text area. There is a maximum of 500 characters for this section.</p>"},{"location":"user-guide/quality-assurance/","title":"Introduction to QA","text":"<p>Quality assurance (QA) in web archiving is the systematic process of verifying that archived web content is accurate, complete, and usable. It often involves checking for broken links, missing content, and ensuring the archived version matches the website website at the time it was crawled, especially sites with dynamic and interactive elements.</p> <p>Quality assurance has often been performed manually, typically by visually comparing crawl results to the live site and clicking on the hyperlinks of a crawled web page. This can be tedious and prone to issues if some interactive elements are overlooked, especially if the live site has changed since the time it was crawled and archived. Browsertrix addresses these potential issues through QA tools that provide immediate feedback on the capture quality of the crawl, so that crawl or replay issues can be identified and resolved promptly.</p>"},{"location":"user-guide/quality-assurance/#overview-of-quality-assurance","title":"Overview of Quality Assurance","text":"<p>With assisted QA, you can analyze any web archive crawled through Browsertrix to compare, replay, and review pages in the web archive.</p> <p>Types of crawls you can review</p> <p>You are able to review and analyze crawls that have been completed or even stopped. You would not be able to review or run an analysis on cancelled crawls, paused crawls or uploaded crawls.</p> <p>At a quick glance, you can tell: </p> <ul> <li>Analysis Status: By default, the status of your analysis will be shown as Not Analyzed because QA analysis does not run automatically. You will need to run analysis if you want an HTML Page match analysis and side-by-side screenshot comparisons. </li> <li>QA Rating: Users can rate crawls if they are Excellent, Good, Fair, Poor, or Bad depending on the quality.</li> <li>Total Analysis Time: Similar to Execution Time (crawl running time), an analysis uses minutes to measure the total runtime of a crawl scaled by the Browser Windows value during a crawl.</li> </ul>"},{"location":"user-guide/quality-assurance/#crawl-results","title":"Crawl Results","text":"<p>You will be automatically given a summarization of your crawl, even without running analysis. </p> <p>You will get a count of all the HTML (HyperText Markup Language) files captured as well as non-HTML files. Non-HTML files include PDFs, Word and text files, images, and other downloadable content that the crawler discovers through clickable links on a page. These files are not analyzed, as they are standalone assets without comparable Web elements. Failed pages did not respond when the crawlers tried to visit them. </p>"},{"location":"user-guide/quality-assurance/#pages","title":"Pages","text":"<p>A Page refers to a Web page. A Web page is a Web document that is accessed in a web browser that would typically be linked together with other Web pages to create a website. </p> <p>You will see a list of all the Web pages crawled featuring its Title, URL, any approval rating and comments done by users of the org.</p>"},{"location":"user-guide/quality-assurance/#review-crawl-analysis","title":"Review Crawl Analysis","text":"<p>From the Quality Assurance tab in the Crawl overview page, you will be able to Review Crawl when you are ready to analyze the quality of the pages from the crawl.</p>"},{"location":"user-guide/review/","title":"Review Crawl Quality","text":"<p>The crawl Review page provides a streamlined interface for quality assurance (QA). Assess and assign a score to the capture quality of an archived item using the heuristics collected during crawl analysis.</p> <p>Crawls can only be reviewed once crawl analysis has been run. If multiple analysis runs have been completed, the page analysis heuristics will be used from the selected analysis run, which are displayed next to the archived item name. The most recent analysis run is selected by default, but you can choose to display data from any other completed or stopped analysis run here as well.</p>"},{"location":"user-guide/review/#heuristics","title":"Heuristics","text":"<p>Crawl analysis generates comparisons across three heuristics that can indicate which pages may be the most problematic.</p>"},{"location":"user-guide/review/#screenshot-comparison","title":"Screenshot Comparison","text":"<p>Screenshots are compared by measuring the perceived difference between color samples and by the intensity of difference between pixels. These metrics are provided by the open source tool Pixelmatch.</p> <p>Discrepancies between crawl and replay screenshots may occur because resources aren't loaded or rendered properly (usually indicating a replay issue).</p> <p>Caveats</p> <p>If many similar pages exhibit similarly poor screenshot comparison scores but look fine in the replay tab, it may be because of page loading time not being long enough during analysis.</p> <p>Some websites may take more time to load than others, including on replay! If the page wasn't given enough time to load during crawl analysis \u2014 because crawl analysis uses the same workflow limit settings as crawling \u2014 increasing the Delay After Page Load workflow setting may yield better screenshot analysis scores, at the cost of extra execution time.</p>"},{"location":"user-guide/review/#extracted-text-comparison","title":"Extracted Text Comparison","text":"<p>Text extracted during crawl analysis is compared to the text extracted during crawling. Text is compared on the basis of Levenshtein distance.</p> <p>Resources not loaded properly on replay may display ReplayWeb.page's <code>Archived Page Not Found</code> error within the extracted text.</p>"},{"location":"user-guide/review/#resource-comparison","title":"Resource Comparison","text":"<p>The resource comparison tab displays a table of resource types, and their HTTP status code count grouped by \"good\" and \"bad\". 2xx &amp; 3xx range status codes are assigned \"good\", 4xx &amp; 5xx range status codes are assigned \"bad\". Bad status codes on crawl indicate that a resource was not successfully captured. Bad status codes on replay that marked good when crawling usually indicate a replay issue.</p> <p>Caveats</p> <p>The number of resources may be higher on replay due to how components of ReplayWeb.page re-write certain request types. A discrepancy alone may not be an indicator that the page is broken, though generally it is a positive sign when the counts are equal.</p> <p>Due to the complicated nature of resource count comparison, this is not available as a sorting option in the pages list.</p>"},{"location":"user-guide/review/#page-review","title":"Page Review","text":"<p>The pages list can be sorted using analysis heuristics to determine the pages that are likely more important to review versus those that might require less attention. After selecting a page to review, looking over the analysis heuristics, and checking them against replay, make a decision about if the page capture was successful or unsuccessful and leave a note about what worked well or what might be problematic.</p> Should I review every page? (Spoiler alert: probably not!) <p>When reviewing a crawl of a site that has many similar pages, all of which exhibit the same error and have similar heuristic scores, it's likely that they all are similarly broken, and you can probably save yourself the trouble. Depending on the website, the heuristic scores may not always be an accurate predictor of quality, but in our testing they are fairly consistent \u2014 consistency being the important factor of this tool. It is up to you, the curator, to make the final quality judgement!</p> <p>Our recommended workflow is as follows: run crawl analysis, examine the most severe issues as highlighted, examine some key examples of common layouts, review any other key pages, and score the crawl accordingly!</p>"},{"location":"user-guide/review/#finish-review","title":"Finish Review","text":"<p>Once a satisfactory amount of pages have been reviewed, press the Finish Review button to give the archived item an overall quality score ranging from \"Excellent!\" to \"Bad\". You can add any additional notes or considerations in the archived item description, which can be edited during this step.</p>"},{"location":"user-guide/running-crawl/","title":"Running Crawls","text":"<p>Running crawls can be modified from the crawl workflow Latest Crawl tab. You may want to modify a running crawl if you find that the workflow is crawling pages that you didn't intend to archive, or if you want a boost of speed.</p>"},{"location":"user-guide/running-crawl/#crawl-workflow-status","title":"Crawl Workflow Status","text":"<p>A crawl workflow that is in progress can be in one of the following states:</p> Status Description  Waiting The workflow can't start running yet but it is queued to run when resources are available.  Starting New resources are starting up. Crawling should begin shortly.  Running The crawler is finding and capturing pages!  Pausing The workflow is in the process of being paused.  Paused The workflow is currently paused.  Resuming The workflow is in the process of resuming after being paused.  Stopping A user has instructed this workflow to stop. Finishing capture of the current pages.  Finishing Downloads The workflow has finished crawling and is finalizing downloads.  Generating WACZ Data is being packaged into WACZ files.  Uploading WACZ WACZ files have been created and are being transferred to storage."},{"location":"user-guide/running-crawl/#watch-crawl","title":"Watch Crawl","text":"<p>You can watch the current state of the browser windows as the crawler visits pages in the Watch tab of Latest Crawl. A list of queued URLs are displayed below in the Upcoming Pages section.</p>"},{"location":"user-guide/running-crawl/#live-exclusion-editing","title":"Live Exclusion Editing","text":"<p>While exclusions can be set before running a crawl workflow, sometimes while crawling the crawler may find new parts of the site that weren't previously known about and shouldn't be crawled, or get stuck browsing parts of a website that automatically generate URLs known as \"crawler traps\".</p> <p>If the crawl queue is filled with URLs that should not be crawled, use the Edit Exclusions button in the Watch tab to instruct the crawler what pages should be excluded from the queue.</p> <p>Exclusions added while crawling are applied to the same exclusion table saved in the workflow's settings and will be used the next time the crawl workflow is run unless they are manually removed.</p>"},{"location":"user-guide/running-crawl/#changing-the-number-of-browser-windows","title":"Changing the Number of Browser Windows","text":"<p>Like exclusions, the number of browser windows can also be adjusted while crawling. On the Watch tab, press the +/- button next to the Running in N browser windows text and set the desired value.</p> <p>Unlike exclusions, this change will not be applied to future workflow runs.</p>"},{"location":"user-guide/running-crawl/#pausing-and-resuming-crawls","title":"Pausing and Resuming Crawls","text":"<p>If you need to reassess or rescope your crawl at any point after it has started, you can pause the running crawl.</p> <p>To pause a running crawl, click the Pause button. The crawl status will change from Running to Pausing as in-progress pages are completed, and then to Paused once the crawler is successful paused. Paused crawls do not continue to accrue execution time.</p> <p>While a crawl is paused, it is possible to replay the pages crawled up to that point and to download the WACZ files from the Latest Crawl tab.</p> <p>To resume a paused crawl, simply click the Resume button. The crawl status will update from Resuming to Running to indicate that the crawler has started crawling again. Any changes to the workflow settings will be applied in the the resumed crawl.</p> Note <p>Paused crawls that are not resumed within 7 days of being paused are automatically updated to Stopped. Once stopped, the crawl is finished and can no longer be resumed.</p>"},{"location":"user-guide/running-crawl/#end-a-crawl","title":"End a Crawl","text":"<p>If a crawl workflow is not crawling websites as intended it may be preferable to end crawling operations and update the crawl workflow's settings before trying again. There are two operations to end crawls, available both on the workflow's details page, or as part of the actions menu in the workflow list.</p>"},{"location":"user-guide/running-crawl/#stopping","title":"Stopping","text":"<p>Stopping a crawl will throw away the crawl queue but otherwise gracefully end the process and save anything that has been collected. Stopped crawls show up in Archived Items and can be used like any other item in the app.</p>"},{"location":"user-guide/running-crawl/#canceling","title":"Canceling","text":"<p>Canceling a crawl will throw away all data collected and immediately end the process. Canceled crawls do not show up in Archived Items, though a record of the runtime and workflow settings can be found in the crawl workflow's list of crawls.</p>"},{"location":"user-guide/signup/","title":"Register For an Account","text":"<p>Already have an account?</p> <p>Skip ahead to running your first crawl.</p> <p>Self-hosting?</p> <p>This guide only applies to hosted Browsertrix accounts. If you're self-hosting Browsertrix, enable open registration to allow others to sign up for an account on your instance.</p> <p>To sign up for Browsertrix, choose a plan. We offer a variety of plans for individuals, teams, and organizations of all sizes.</p>"},{"location":"user-guide/signup/#register-your-org","title":"Register Your Org","text":"<p>After you finish setting up your subscription, you'll receive an email from Webrecorder with a link to set up your org (organization.)</p> <p>Choose an org name that's unique and memorable, like the name of your company or organization. Your org URL identifier will determine the unique URL of the org's home page.</p>"},{"location":"user-guide/signup/#configure-your-user-account","title":"Configure Your User Account","text":"<p>Create a password and display name. Your display name is the name that is visible to other org members. For example, your display name will be visible in crawl workflows that you create or run.</p> <p>Once you've completed this step, you're in! Next, familiarize yourself with the org overview, head over to crawl workflows, or invite team members.</p>"},{"location":"user-guide/signup/#cant-complete-sign-up-or-have-a-question","title":"Can't complete sign-up or have a question?","text":"<p>Don't hesitate to reach out to us if you encounter any issues while signing up.</p>"},{"location":"user-guide/user-settings/","title":"Change Your Name, Email, or Password","text":""},{"location":"user-guide/user-settings/#display-name","title":"Display Name","text":"<p>Specify how you want your name to be shown to other org members. For example, your display name will be shown with the crawl workflows that you run. This display name will be used for all orgs that you're a part of.</p>"},{"location":"user-guide/user-settings/#email","title":"Email","text":"<p>Update the email that you use to log in and receive Browsertrix emails from.</p>"},{"location":"user-guide/user-settings/#password","title":"Password","text":"<p>Update the password that you use to login. For your security, we enforce strong passwords. For more information on we secure your account by enforcing strong passwords, see zxcvbn.</p>"},{"location":"user-guide/workflow-setup/","title":"Crawl Workflow Settings","text":"<p>One of the key features of Browsertrix is the ability to refine crawler settings to the exact specifications of your crawl and website.</p> <p>Changes to a setting will only apply to subsequent crawls.</p> <p>Crawl settings are shown in the crawl workflow detail Settings tab and in the archived item Crawl Settings tab.</p>"},{"location":"user-guide/workflow-setup/#scope","title":"Scope","text":"<p>Specify the range and depth of your crawl.</p> <p>Crawl scopes are categorized as a Page Crawl or Site Crawl:</p>"},{"location":"user-guide/workflow-setup/#page-crawl","title":"Page Crawl","text":"<p>Choose one of these crawl scopes if you know the URL of every page you'd like to crawl and don't need to include any additional pages beyond one hop out.</p> <p>A Page Crawl workflow can be simpler to configure, since you don't need to worry about configuring the workflow to exclude parts of the website that you may not want to archive.</p> Page Crawl Use Cases <ul> <li>You want to archive a social media post (<code>Single Page</code>)</li> <li>You have a list of URLs that you can copy-and-paste (<code>List of Pages</code>)</li> <li>You want to include URLs with different domain names in the same crawl (<code>List of Pages</code>)</li> </ul>"},{"location":"user-guide/workflow-setup/#site-crawl","title":"Site Crawl","text":"<p>Choose one of these crawl scopes to have the the crawler automatically find pages based on a domain name, start page URL, or directory on a website.</p> <p>Site Crawl workflows are great for advanced use cases where you don't need (or want) to know every single URL of the website that you're archiving.</p> Site Crawl Use Cases <ul> <li>You're archiving a subset of a website, like everything under website.com/your-username (<code>Pages in Same Directory</code>)</li> <li>You're archiving an entire website and external pages linked to from the website (<code>Pages on Same Domain</code> + Include Any Linked Page checked)</li> </ul>"},{"location":"user-guide/workflow-setup/#crawl-scope-options","title":"Crawl Scope Options","text":""},{"location":"user-guide/workflow-setup/#page-crawl_1","title":"Page Crawl","text":""},{"location":"user-guide/workflow-setup/#single-page","title":"Single Page","text":"Crawls a single URL and does not include any linked pages."},{"location":"user-guide/workflow-setup/#list-of-pages","title":"List of Pages","text":"<p>Crawls a list of specified URLs.</p> <p>Select one of two options to provide a list of URLs:</p>"},{"location":"user-guide/workflow-setup/#enter-urls","title":"Enter URLs","text":"If the list is small enough, 100 URLs or less, the URLs can be entered directly into the text area. If a large list is pasted into the textbox, it will be converted into an uploaded URL list and attached to the workflow."},{"location":"user-guide/workflow-setup/#upload-url-list","title":"Upload URL List","text":"A longer list of URLs can be provided as a text file, containing one URL per line. The text file may not exceed 25MB, but there is no limit to the number of URLs in the file. Once a file is added, a link will be provided to view the file (but not edit it). To change the file, a new file can be uploaded in its place. <p>For both options, each line should contain a valid URL (starting with https:// or http://). Invalid or duplicate URLs will be skipped. The crawl will fail if the list contains no valid URLs or if the file is not a list of URLs.</p> <p>While the uploaded text file can contain an unlimited number of URLs, the crawl will still be limited by the page limit for the workflow or organization - URLs beyond the limit will not be crawled.</p> <p>If both a list of entered list and an uploaded file are provided, the currently selected option will be used.</p>"},{"location":"user-guide/workflow-setup/#in-page-links","title":"In-Page Links","text":"<p>Crawls only the specified URL and treats linked sections of the page as distinct pages.</p> <p>Any link that begins with the Crawl Start URL followed by a hashtag symbol (<code>#</code>) and then a string is considered an in-page link. This is commonly used to link to a section of a page. For example, because the \"Scope\" section of this guide is linked by its heading as <code>/user-guide/workflow-setup/#scope</code> it would be treated as a separate page under the In-Page Links scope.</p> <p>This scope can also be useful for crawling websites that are single-page applications where each page has its own hash, such as <code>example.com/#/blog</code> and <code>example.com/#/about</code>.</p>"},{"location":"user-guide/workflow-setup/#site-crawl_1","title":"Site Crawl","text":""},{"location":"user-guide/workflow-setup/#pages-in-same-directory","title":"Pages in Same Directory","text":"This scope will only crawl pages in the same directory as the Crawl Start URL. If <code>example.com/path</code> is set as the Crawl Start URL, <code>example.com/path/path2</code> will be crawled but <code>example.com/path3</code> will not."},{"location":"user-guide/workflow-setup/#pages-on-same-domain","title":"Pages on Same Domain","text":"This scope will crawl all pages on the domain entered as the Crawl Start URL however it will ignore subdomains such as <code>subdomain.example.com</code>."},{"location":"user-guide/workflow-setup/#pages-on-same-domain-subdomains","title":"Pages on Same Domain + Subdomains","text":"This scope will crawl all pages on the domain and any subdomains found. If <code>example.com</code> is set as the Crawl Start URL, both pages on <code>example.com</code> and <code>subdomain.example.com</code> will be crawled."},{"location":"user-guide/workflow-setup/#custom-page-prefix","title":"Custom Page Prefix","text":"This scope will crawl the Crawl Start URL and then include only those pages that begin with the URLs listed in URL Prefixes in Scope."},{"location":"user-guide/workflow-setup/#crawl-start-url-urls-to-crawl","title":"Crawl Start URL / URL(s) to Crawl","text":"<p>This is the URL used by the crawler to initiate the crawling process. The URL input may be labeled Crawl Start URL or URL(s) to Crawl depending on which crawl scope is used:</p> Crawl Scope Label Description Single Page URL\u00a0to\u00a0Crawl The crawler will visit only this URL. List of Pages URLs\u00a0to\u00a0Crawl The crawler will visit each URL specified in the text list or file. In-Page LinksPages in Same DirectoryPages on Same DomainPages on Same Domain + SubdomainsCustom Page Prefix Crawl\u00a0Start\u00a0URL The crawler will visit this URL as its starting point and use this URL to collect information on which linked pages it should also visit. <p>URLs must follow valid URL syntax. For example, if you're crawling a page that can be accessed on the public internet, your URL should start with <code>http://</code> or <code>https://</code>.</p> <p>Refer to a specific Crawl Scope option for details on how each crawl scope interacts with this URL.</p> Crawling with HTTP basic auth <p>All crawl scopes support HTTP Basic Auth which can be provided as part of the URL, for example: <code>https://username:password@example.com</code>.</p> <p>These credentials WILL BE WRITTEN into the archive. We recommend exercising caution and only archiving with dedicated archival accounts, changing your password or deleting the account when finished.</p>"},{"location":"user-guide/workflow-setup/#skip-pages-disallowed-by-robotstxt","title":"Skip Pages Disallowed By Robots.txt","text":"<p>When enabled, the crawler will check for a Robots Exclusion Protocol file at /robots.txt for each host encountered during crawling and skip any pages that are disallowed by the rules found therein.</p>"},{"location":"user-guide/workflow-setup/#include-any-linked-page","title":"Include Any Linked Page","text":"<p>When enabled, the crawler will visit all the links it finds within each URL defined in the URL input field under Crawl Scope.</p> Crawling tags &amp; search queries with Page List crawls <p>This setting can be useful for crawling a list of specific pages and pages they link to, such as a list of search queries. For example, you can add a list of multiple URLs such as: <code>https://example.com/search?q=search_this</code>, <code>https://example.com/search?q=also_search_this</code>, etc... to the URLs to Crawl text box and enable Include Any Linked Page to crawl all the content present on these search query pages.</p>"},{"location":"user-guide/workflow-setup/#fail-crawl-if-not-logged-in","title":"Fail Crawl if Not Logged In","text":"<p>When enabled, the crawl will fail if a page behavior detects the presence or absence of content on supported pages indicating that the browser is not logged in.</p> <p>For details about which websites are supported and how to add this functionality to your own custom behaviors, see the Browsertrix Crawler documentation for Fail on Content Check.</p>"},{"location":"user-guide/workflow-setup/#fail-crawl-on-failed-url","title":"Fail Crawl on Failed URL","text":"<p>When enabled, the crawler will fail the entire crawl if any of the provided URLs are invalid or unsuccessfully crawled. The resulting archived item will have a status of \"Failed\".</p>"},{"location":"user-guide/workflow-setup/#max-depth-in-scope","title":"Max Depth in Scope","text":"<p>Instructs the crawler to stop visiting new links past a specified depth.</p>"},{"location":"user-guide/workflow-setup/#url-prefixes-in-scope","title":"URL Prefixes in Scope","text":"<p>When using a scope of <code>Custom Page Prefix</code>, this field accepts URLs or domains that will be crawled if URLs that lead to them are found.</p> <p>By default, URL Prefixes in Scope will be prefilled with the Crawl Start URL up to the last slash (<code>/</code>). For example, if <code>https://example.com/path/page</code> is set as the Crawl Start URL, <code>https://example.com/path/</code> will be automatically added to URL Prefixes in Scope. This URL prefix can then be removed or modified as needed.</p> <p>This field can also be useful for crawling websites that span multiple domains such as <code>https://example.org</code> and <code>https://example.net</code>. To crawl websites outside of scope for scope types other than <code>Custom Page Prefix</code>, see Additional Pages.</p>"},{"location":"user-guide/workflow-setup/#include-any-linked-page-one-hop-out","title":"Include Any Linked Page (\"one hop out\")","text":"<p>When enabled, the crawler bypasses the Crawl Scope setting to visit links it finds in each page within scope. The crawler will not visit links it finds in the pages found outside of scope (hence only \"one hop out\".)</p> <p>This can be useful for capturing links on a page that lead outside the website that is being crawled but should still be included in the archive for context.</p>"},{"location":"user-guide/workflow-setup/#check-for-sitemap","title":"Check For Sitemap","text":"<p>When enabled, the crawler will check for a sitemap at /sitemap.xml and use it to discover pages to crawl if found. It will not crawl pages found in the sitemap that do not meet the crawl's scope settings or limits.</p> <p>This can be useful for discovering and capturing pages on a website that aren't linked to from the seed and which might not otherwise be captured.</p>"},{"location":"user-guide/workflow-setup/#link-selectors","title":"Link Selectors","text":"<p>Instructs the crawler which HTML elements should be used to extract URLs, i.e. considered a \u201clink.\u201d By default, the crawler checks the <code>href</code> value of all anchor (<code>&lt;a&gt;</code>) elements on a page.</p> <p>Specifying a custom link selector can be useful for websites that hyperlink to pages using an element other than the standard <code>&lt;a&gt;</code> tag, or use an attribute other than <code>href</code> to specify the URL.</p> <p>For example, for a page with the given HTML markup:</p> <pre><code>&lt;button class=\"link\" data-href=\"/blog\"&gt;Blog&lt;/button&gt;\n&lt;button class=\"link\" data-href=\"/about\"&gt;About&lt;/button&gt;\n</code></pre> <p>The CSS Selector for a custom link selector could be <code>button.link</code> and its Link Attribute would be <code>data-href</code>.</p> <p>See Basic CSS selectors (MDN) for examples of valid CSS selectors.</p>"},{"location":"user-guide/workflow-setup/#additional-pages","title":"Additional Pages","text":"<p>A list of page URLs outside of the Crawl Scope to include in the crawl.</p>"},{"location":"user-guide/workflow-setup/#exclude-pages","title":"Exclude Pages","text":"<p>The exclusions table will instruct the crawler to ignore links it finds on pages where all or part of the link matches an exclusion found in the table. The table is only available in Page List crawls when Include Any Linked Page is enabled.</p> <p>This can be useful for avoiding crawler traps \u2014 sites that may automatically generate pages such as calendars or filter options \u2014 or other pages that should not be crawled according to their URL.</p>"},{"location":"user-guide/workflow-setup/#matches-text","title":"Matches text","text":"<p>Will perform simple matching of entered text and exclude all URLs where matching text is found.</p> <p>e.g: If <code>about</code> is entered, <code>example.com/aboutme/</code> will not be crawled.</p>"},{"location":"user-guide/workflow-setup/#regex","title":"Regex","text":"<p>Regular expressions (Regex) can also be used to perform more complex matching.</p> <p>e.g: If <code>\\babout\\/?\\b</code> is entered, <code>example.com/about/</code> will not be crawled however <code>example.com/aboutme/</code> will be crawled.</p>"},{"location":"user-guide/workflow-setup/#crawl-limits","title":"Crawl Limits","text":"<p>Enforce maximum limits on your crawl.</p>"},{"location":"user-guide/workflow-setup/#max-pages","title":"Max Pages","text":"<p>Adds a hard limit on the number of pages that will be crawled. The crawl will be gracefully stopped after this limit is reached.</p>"},{"location":"user-guide/workflow-setup/#crawl-time-limit","title":"Crawl Time Limit","text":"<p>The crawl will be gracefully stopped after this set period of elapsed time.</p>"},{"location":"user-guide/workflow-setup/#crawl-size-limit","title":"Crawl Size Limit","text":"<p>The crawl will be gracefully stopped after reaching this set size in GB.</p>"},{"location":"user-guide/workflow-setup/#page-behavior","title":"Page Behavior","text":"<p>Customize how and when the browser performs specific operations on a page.</p> <p>Behaviors</p> <p>Behaviors are browser operations that can be enabled for additional page interactivity.</p>"},{"location":"user-guide/workflow-setup/#autoscroll","title":"Autoscroll","text":"<p>When enabled, the browser will automatically scroll to the end of the page.</p>"},{"location":"user-guide/workflow-setup/#autoclick","title":"Autoclick","text":"<p>When enabled, the browser will automatically click on all link-like elements.</p> <p>When clicking a link-like element that would normally result in navigation, autoclick will only record the click and prevent navigation away from the current page.</p> Autoclick use cases <p>This behavior can be helpful for:</p> <ul> <li> <p>Websites that use anchor links (<code>&lt;a&gt;</code>) in non-standard ways, such as by using JavaScript in place of the standard <code>href</code> attribute to create a hyperlink.</p> </li> <li> <p>Websites that use <code>&lt;a&gt;</code> in place of a <code>&lt;button&gt;</code> to reveal in-page content.</p> </li> </ul>"},{"location":"user-guide/workflow-setup/#click-selector","title":"Click Selector","text":"<p>When autoclick is enabled, you can customize which element is automatically clicked by specifying a CSS selector.</p> <p>See Basic CSS selectors (MDN) for examples of valid CSS selectors.</p>"},{"location":"user-guide/workflow-setup/#use-custom-behaviors","title":"Use Custom Behaviors","text":"<p>Enable custom behaviors to add your own behavior scripts. See Browser Behaviors crawler documentation on creating custom behaviors.</p> <p>Custom behaviors can be specified as:</p>"},{"location":"user-guide/workflow-setup/#url","title":"URL","text":"<p>A URL for a single JavaScript or JSON behavior file to download. This should be a URL that the crawler has access to. The workflow editor will validate that the supplied URL can be reached.</p>"},{"location":"user-guide/workflow-setup/#git-repository","title":"Git repository","text":"<p>A URL for a public Git repository containing one or more behavior files. Optionally, you can specify a branch and/or a relative path within the repository to specify exactly which behavior files within the repository should be used. The workflow editor will validate that the URL can be reached and is a Git repository. If a branch name is specified, the workflow editor will also validate that the branch exists in the Git repository.</p> <p>Page Timing</p> <p>Page timing gives you more granular control over how long the browser should stay on a page and when behaviors should run on a page. Add limits to decrease the amount of time the browser spends on a page, and add delays to increase the amount of time the browser waits on a page. Adding delays will increase the total amount of time spent on a crawl and may impact your overall crawl minutes.</p>"},{"location":"user-guide/workflow-setup/#page-load-limit","title":"Page Load Limit","text":"<p>Limits amount of elapsed time to wait for a page to load. Behaviors will run after this timeout only if the page is partially or fully loaded.</p>"},{"location":"user-guide/workflow-setup/#delay-after-page-load","title":"Delay After Page Load","text":"<p>Waits on the page after initial HTML page load for a set number of seconds prior to moving on to next steps such as link extraction and behaviors. Can be useful with pages that are slow to load page contents.</p>"},{"location":"user-guide/workflow-setup/#behavior-limit","title":"Behavior Limit","text":"<p>Limits amount of elapsed time behaviors have to complete.</p>"},{"location":"user-guide/workflow-setup/#delay-before-next-page","title":"Delay Before Next Page","text":"<p>Waits on the page for a set number of seconds before unloading the current page. If any behaviors are enabled, this delay will take place after all behaviors have finished running. This can be helpful to avoid rate limiting.</p>"},{"location":"user-guide/workflow-setup/#browser-settings","title":"Browser Settings","text":"<p>Configure the browser used to visit URLs during the crawl.</p>"},{"location":"user-guide/workflow-setup/#browser-profile","title":"Browser Profile","text":"<p>Sets the Browser Profile to be used for this crawl.</p>"},{"location":"user-guide/workflow-setup/#crawler-proxy-server","title":"Crawler Proxy Server","text":"<p>This setting will be shown if the organization supports multiple proxies.</p> <p>Sets the proxy server that Browsertrix Crawler will direct traffic through while crawling. When a proxy is selected, crawled websites will see traffic as coming from the IP address of the proxy rather than where Browsertrix Crawler is deployed.</p> <p>If a Browser Profile is specified, this field will be disabled and the proxy settings of the browser profile will be used when crawling. This prevents potential crawl failures that result from conflicting proxies.</p>"},{"location":"user-guide/workflow-setup/#browser-windows","title":"Browser Windows","text":"<p>Sets the number of browser windows that are used to visit webpages while crawling. Increasing the number of browser windows will speed up crawls by capturing more pages in parallel.</p> <p>There are some trade-offs:</p> <ul> <li>This may result in a higher chance of getting rate limited due to the increase in traffic sent to the website.</li> <li>More execution minutes will be used per-crawl.</li> </ul>"},{"location":"user-guide/workflow-setup/#crawler-release-channel","title":"Crawler Release Channel","text":"<p>This setting will be shown if the organization supports multiple release channels.</p> <p>Sets the release channel of Browsertrix Crawler. Crawls of this workflow will use the latest crawler version from the selected release channel. Generally reserved for advanced use cases, such as enabling experimental features that may not have been fully tested yet.</p>"},{"location":"user-guide/workflow-setup/#block-ads-by-domain","title":"Block Ads by Domain","text":"<p>Will prevent any content from the domains listed in Steven Black's Unified Hosts file (ads &amp; malware) from being captured by the crawler.</p>"},{"location":"user-guide/workflow-setup/#save-local-and-session-storage","title":"Save Local and Session Storage","text":"<p>When enabled, instructs the crawler to save the browser's <code>localStorage</code> and <code>sessionStorage</code> data for each page in the web archive as part of the <code>WARC-JSON-Metadata</code> field. This option may be necessary to properly archive and replay certain websites. Use caution when sharing WACZ files created with this option enabled, as the saved browser storage may contain sensitive information.</p>"},{"location":"user-guide/workflow-setup/#user-agent","title":"User Agent","text":"<p>Sets the browser's user agent in outgoing requests to the specified value. If left blank, the crawler will use the Brave browser's default user agent. For a list of common user agents see useragents.me.</p> Using custom user agents to get around restrictions <p>Despite being against best practices, some websites will block specific browsers based on their user agent: a string of text that browsers send web servers to identify what type of browser or operating system is requesting content. If Brave is blocked, using a user agent string of a different browser (such as Chrome or Firefox) may be sufficient to convince the website that a different browser is being used.</p> <p>User agents can also be used to voluntarily identify your crawling activity, which can be useful when working with a website's owners to ensure crawls can be completed successfully. We recommend using a user agent string similar to the following, replacing the <code>orgname</code> and URL comment with your own:</p> <pre><code>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.3 orgname.browsertrix (+https://example.com/crawling-explination-page)\n</code></pre> <p>If you have no webpage to identify your organization or statement about your crawling activities available as a link, omit the bracketed comment section at the end entirely.</p> <p>This string must be provided to the website's owner so they can allowlist Browsertrix to prevent it from being blocked.</p>"},{"location":"user-guide/workflow-setup/#language","title":"Language","text":"<p>Sets the browser's language setting. Useful for crawling websites that detect the browser's language setting and serve content accordingly.</p>"},{"location":"user-guide/workflow-setup/#scheduling","title":"Scheduling","text":"<p>Automatically start crawls periodically on a daily, weekly, or monthly schedule.</p>"},{"location":"user-guide/workflow-setup/#crawl-schedule-type","title":"Crawl Schedule Type","text":""},{"location":"user-guide/workflow-setup/#run-on-a-recurring-basis","title":"Run on a Recurring Basis","text":"<p>When selected, additional configuration options for instructing the system when to run the crawl will be shown. If a crawl is already running when the schedule is set to activate it, the scheduled crawl will not run.</p> <p>Tip: Scheduling crawl workflows with logged-in browser profiles</p> <p>Some websites will log users out after a set period of time. This can cause issues with scheduled crawl workflows\u2014which will run even if the selected browser profile has been logged out.</p> <p>For some websites, a short schedule frequency can help keep the browser profile logged in by regularly and automatically refreshing the login session. A separate crawl workflow could be created for this purpose. We recommend manually checking the profile periodically to ensure that it is still logged in.</p>"},{"location":"user-guide/workflow-setup/#no-schedule","title":"No Schedule","text":"When selected, the configuration options that have been set will be saved but the system will not do anything with them unless manually instructed."},{"location":"user-guide/workflow-setup/#frequency","title":"Frequency","text":"<p>Set how often a scheduled crawl will run.</p>"},{"location":"user-guide/workflow-setup/#options","title":"Options","text":"<p>All option support specifying the specific hour and minute the crawl should run.</p>"},{"location":"user-guide/workflow-setup/#daily","title":"Daily","text":"<p>Run crawl once every day.</p>"},{"location":"user-guide/workflow-setup/#weekly","title":"Weekly","text":"<p>Run crawl once every week.</p>"},{"location":"user-guide/workflow-setup/#monthly","title":"Monthly","text":"<p>Run crawl once every month.</p>"},{"location":"user-guide/workflow-setup/#custom","title":"Custom","text":"<p>Run crawl at a custom interval, such as hourly or yearly. See Cron Schedule for details.</p>"},{"location":"user-guide/workflow-setup/#day","title":"Day","text":"<p>Sets the day of the week for which crawls scheduled with a <code>Weekly</code> Frequency will run.</p>"},{"location":"user-guide/workflow-setup/#date","title":"Date","text":"<p>Sets the date of the month for which crawls scheduled with a <code>Monthly</code> Frequency will run.</p>"},{"location":"user-guide/workflow-setup/#start-time","title":"Start Time","text":"<p>Sets the time that the scheduled crawl will start according to your current timezone.</p>"},{"location":"user-guide/workflow-setup/#cron-schedule","title":"Cron Schedule","text":"<p>When using a <code>Custom</code> Frequency, a custom schedule can be specified by using a Cron expression or supported macros.</p> <p>Cron expressions should follow the Unix Cron format:</p> Position * * * * * Description minute hour day of the month month day of the week Possible Values 0 - 59 0 - 23 1 - 31 1 - 12 0 - 6or <code>sun</code>, <code>mon</code>, <code>tue</code>, <code>wed</code>, <code>thu</code>, <code>fri</code>, <code>sat</code> <p>For example, <code>0 0 31 12 *</code> would run a crawl on December 31st every year and <code>0 0 * * fri</code> would run a crawl every Friday at midnight.</p> <p>Additionally, the following macros are supported:</p> Value Description <code>@yearly</code> Run once a year at midnight of 1 January <code>@monthly</code> Run once a month at midnight of the first day of the month <code>@weekly</code> Run once a week at midnight on Sunday <code>@daily</code> Run once a day at midnight <code>@hourly</code> Run once an hour at the beginning of the hour <p>You can use a tool like crontab.guru to check Cron syntax validity and view common expressions.</p> <p>Cron schedules are always in UTC.</p>"},{"location":"user-guide/workflow-setup/#collections","title":"Collections","text":""},{"location":"user-guide/workflow-setup/#auto-add-to-collection","title":"Auto-Add to Collection","text":"<p>Search for and specify collections that this crawl workflow should automatically add archived items to as soon as crawling finishes. Canceled and Failed crawls will not be added to collections.</p>"},{"location":"user-guide/workflow-setup/#metadata","title":"Metadata","text":"<p>Describe and organize your crawl workflow and the resulting archived items.</p>"},{"location":"user-guide/workflow-setup/#name","title":"Name","text":"<p>Allows a custom name to be set for the workflow. If no name is set, the workflow's name will be set to the first URL to crawl specified in Scope. For Page List crawls, the workflow name may show an added <code>+N</code> where <code>N</code> represents the number of URLs in addition to the first URL to crawl.</p>"},{"location":"user-guide/workflow-setup/#description","title":"Description","text":"<p>Leave optional notes about the workflow's configuration.</p>"},{"location":"user-guide/workflow-setup/#tags","title":"Tags","text":"<p>Apply tags to the workflow. Tags applied to the workflow will propagate to every crawl created with it at the time of crawl creation.</p>"},{"location":"user-guide/browser-profiles/browser-profiles-overview/","title":"Intro to Browser Profiles","text":"<p>Browser profiles are saved instances of a web browsing session that can be used to configure a website before it is crawled.</p>"},{"location":"user-guide/browser-profiles/browser-profiles-overview/#common-use-cases","title":"Common Use Cases","text":""},{"location":"user-guide/browser-profiles/browser-profiles-overview/#social-media-sign-in","title":"Social Media Sign In","text":"<p>Pre-configure a social media site to be logged in so that the crawler can access content that can only be viewed by logged-in users.</p> <p>Best Practices: Use an account created specifically for archiving a website</p> <p>Websites may require user registration to view content at URLs that are otherwise public. This practice is sometimes referred to as a login wall. Login walls are commonly used by social media and publishing platforms.</p> <p>We recommend creating dedicated accounts when archiving any website that requires a user account. Although dedicated accounts are not required to benefit from browser profiles, they can address the following potential issues:</p> <ul> <li> <p>While usernames and passwords are never saved by Browsertrix, the private tokens that enable access to logged in content are stored. Thus, anyone with access to your Browsertrix account, intentional or malicious, may be able to access the logged in content.</p> </li> <li> <p>Some websites may rate limit or lock accounts for reasons they deem to be suspicious, such as logging in from a new geographical location or if the site determines crawls to be robot activity.</p> </li> <li> <p>Personalized data such as cookies, location, etc. may be included in the resulting crawl.</p> </li> <li> <p>The logged in interface may display unwanted personally identifiable information such as a username or profile picture.</p> </li> </ul> <p>An exception to this practice is if your goal is to archive personalized or private content accessible only from designated accounts. In these instances we recommend changing the account's password after crawling is complete.</p>"},{"location":"user-guide/browser-profiles/browser-profiles-overview/#hide-popup-prompts","title":"Hide Popup Prompts","text":"<p>Websites may prompt users for a number of reasons before displaying the rest of the page, such as for age verification, informed consent requirements, or geographical location. Configure a browser profile to accept, dismiss, or otherwise hide these dialogs so that the content behind them is visible to the crawler.</p>"},{"location":"user-guide/browser-profiles/configure-sites/","title":"Configure Sites","text":"<p>Websites are configured through a temporary browser that is embedded directly in the Browsertrix interface. Every website that is visited using the embedded browser is added to the list of Saved Sites. When the embedded browser session ends, personalized data from the sites are collected into a profile. This profile of preconfigured sites can then be saved and used by multiple crawl workflows.</p> <p>The embedded browser is used during the process of creating a new browser profile and when editing an existing profile.</p>"},{"location":"user-guide/browser-profiles/configure-sites/#use-cases","title":"Use Cases","text":""},{"location":"user-guide/browser-profiles/configure-sites/#website-sign-in","title":"Website Sign In","text":"<p>To crawl content as a logged in user, load the website you intend to archive in the embedded browser and sign in as you would on any other browser. Once the account has been logged in, confirm by accessing a page on the site that the crawler should have access to. You may need to periodically log in again as websites may log users out after a certain period of time.</p> <p>Tip: Crawl regularly to stay logged in</p> <p>Regularly running crawl workflows that use a browser profile can help to reduce the frequency with which logouts occur on some websites. Data such as cookies and sessions may be refreshed during crawling, and Browsertrix will automatically update the browser profile with this data when each crawl successfully finishes.</p>"},{"location":"user-guide/browser-profiles/configure-sites/#hide-popups","title":"Hide Popups","text":"<p>Load the website you intend to archive in the embedded browser and accept or otherwise dismiss the prompt. If the developers of the website have built the site in such a way that the result of your interaction is saved, the popup should remain hidden at crawl time. This can be confirmed by exiting the embedded browser session and then loading the site again.</p>"},{"location":"user-guide/browser-profiles/configure-sites/#customize-the-crawling-browser","title":"Customize the Crawling Browser","text":"<p>The embedded browser used to configure profiles is the same browser behind Browsertrix\u2019s high-fidelity crawls. This enables advanced use cases like using a browser profile to customize the browser at crawl time. To view all available browser settings, load any site in the profile and then navigate to <code>brave://settings</code> in the embedded browser.</p> <p>Advanced Use Case: Proceed with caution</p> <p>Customizing the crawler browser is for advanced use cases and it is not generally recommended to change these settings. We offer crawl-time browser customization like ad blocking and language in workflow settings. Changing browser settings directly in the profile may result in conflicting settings that are difficult to troubleshoot. If using this advanced feature, we recommend adding clear metadata to the browser profile that describes the change.</p> Example: Blocking page resources with Brave's Shields <p>Whereas the crawler's scoping settings can be used to define which pages should be crawled, Brave's Shields feature can block resources on pages from being loaded. By default, Shields will block EasyList's cookie list but it can be set to block a number of other included lists under Brave <code>Settings &gt; Shields &gt; Filter Lists</code>.</p> <p>Custom Filters can also be useful for blocking sites with resources that aren't included in one of the known block lists.</p> <p>The uBlock Origin filter syntax can be used for more specificity over what in-page resources should be blocked.</p> <p>All of the browser's ad blocking and privacy features can be used in combination with the Block Ads by Domain crawler setting.</p>"},{"location":"user-guide/browser-profiles/configure-sites/#saving-the-profile","title":"Saving the Profile","text":"<p>After you are done interacting with the embedded browser, press Save Profile (or Create Profile for new browser profiles.)</p>"},{"location":"user-guide/browser-profiles/configure-sites/#saved-sites","title":"Saved Sites","text":"<p>All sites that are loaded in the embedded browser and then saved will appear in the Saved Sites list. Select a site in the list to view or reconfigure the site in the embedded browser.</p>"},{"location":"user-guide/browser-profiles/configure-sites/#load-new-url","title":"Load New URL","text":"<p>You may want to load a URL that is not listed in the Saved Sites to preview how a page may appear to the crawler, or to add a new site. Due to the nature of the embedded browser, it can be difficult to navigate between different websites if there are no hyperlinks between them. The easiest way to load a new URL is to press Load New URL from the browser profile page and enter the URL.</p> <p>Although browser profiles have no limit on the number of saved sites, we recommend one site per browser profile to make troubleshooting crawls easier. An exception is when using a URL List workflow to crawl multiple websites that require a profile, as we only allow one browser profile per workflow.</p>"},{"location":"user-guide/browser-profiles/create-browser-profile/","title":"Create a New Browser Profile","text":"<p>To create a new browser profile, press New Browser Profile on the Browser Profiles page.</p>"},{"location":"user-guide/browser-profiles/create-browser-profile/#new-browser-profile-settings","title":"New Browser Profile Settings","text":"<p>The URL of the first page to visit in the embedded browser. For example, the login page for a social media website.</p> <p>A custom name for the browser profile. The domain name of the Primary Site URL will be used if this field is left blank.</p> Depending on your organization settings, additional settings may be available: <p>The proxy server to be used by the embedded browser as well as any crawl that uses this profile.</p> <p>Implication for crawl workflows using proxies</p> <p>When a browser profile is added to a crawl workflow, the browser profile\u2019s proxy setting will take precedence over the crawl workflow\u2019s Crawler Proxy Server setting. This prevents potential crawl failures that result from conflicting proxies.</p> <p>For advanced use cases, you can specify a Browsertrix Crawler release that contains another version of the embedded browser, such as a beta version that may contain experimental features.</p> <p>Press Start Browser to load the temporary embedded browser used to configure sites. It may take a few moments for the embedded browser to load. The browser profile will not be saved until Create Profile is pressed.</p>"},{"location":"user-guide/browser-profiles/create-browser-profile/#primary-site-url","title":"Primary Site URL","text":""},{"location":"user-guide/browser-profiles/create-browser-profile/#profile-name","title":"Profile Name","text":""},{"location":"user-guide/browser-profiles/create-browser-profile/#proxy-server","title":"Proxy Server","text":""},{"location":"user-guide/browser-profiles/create-browser-profile/#crawler-release-channel","title":"Crawler Release Channel","text":""},{"location":"user-guide/browser-profiles/edit-browser-profile/","title":"Edit Browser Profile","text":"<p>Sometimes websites will log users out by expiring cookies or login sessions after a period of time. Although running crawls on a regular basis can help keep websites logged in for longer, most websites will log users out after an extended period of time, be it a week or 30 days. In this case, the browser profile may not behave as expected at crawl time and will need to be reconfigured.</p> <p>To check or update the profile, go to the browser profile page and select Load Profile from the action menu.</p> <p>Tip: Fail crawls early to identify logged-out profiles</p> <p>Enabling Fail Crawl If Not Logged In on a workflow can help identify which profiles need attention and prevent adding unwanted logged-out content to a collection.</p>"},{"location":"user-guide/browser-profiles/edit-browser-profile/#load-edit-profile-settings","title":"Load / Edit Profile Settings","text":"<p>The primary site that is configured in the browser profile. A new primary site can be configured by choosing New Site in the dropdown.</p> <p>If the browser profile is in use by crawl workflows with a crawl start URL that is not a saved site, a section titled Suggestions from Related Workflows with suggested options will be displayed in the dropdown.</p> <p>The URL of the first page in the primary site to visit in the embedded browser. For example, the login page for a social media website.</p> <p>If checked, all previously saved sites and their associated data will be removed from the browser profile. If your organization supports proxies, this will also enable choosing a different proxy server.</p> Depending on your organization settings, additional settings may be available: <p>The proxy server to be used by the embedded browser as well as any crawl that uses this profile.</p> Implication for crawl workflows using proxies <p>When a browser profile is added to a crawl workflow, the browser profile\u2019s proxy setting will take precedence over the crawl workflow\u2019s Crawler Proxy Server setting. This prevents potential crawl failures that result from conflicting proxies.</p> <p>For advanced use cases, you can specify a Browsertrix Crawler release that contains another version of the embedded browser, such as a beta version that may contain experimental features.</p> <p>Press Start Browser to load the temporary embedded browser. It may take a few moments for the embedded browser to load.</p> <p>When finished, press the Save Profile button to return to the profile's details page. Profiles are automatically backed up on save if replica storage locations are configured.</p>"},{"location":"user-guide/browser-profiles/edit-browser-profile/#primary-site","title":"Primary Site","text":""},{"location":"user-guide/browser-profiles/edit-browser-profile/#url-to-load","title":"URL to Load","text":""},{"location":"user-guide/browser-profiles/edit-browser-profile/#reset-previous-configuration-on-save","title":"Reset previous configuration on save","text":""},{"location":"user-guide/browser-profiles/edit-browser-profile/#proxy-server","title":"Proxy Server","text":""},{"location":"user-guide/browser-profiles/edit-browser-profile/#crawler-release-channel","title":"Crawler Release Channel","text":""},{"location":"user-guide/browser-profiles/edit-browser-profile/#edit-browser-profile-metadata","title":"Edit Browser Profile Metadata","text":"To edit the name, description, and tags, select Edit Metadata from the action menu on the browser profile page. <p>A custom name for the browser profile.</p> <p>A short description of the browser profile.</p> <p>Tag the browser profile with additional metadata like category or keywords. Tags are displayed on the browser profiles list page.</p>"},{"location":"user-guide/browser-profiles/edit-browser-profile/#name","title":"Name","text":""},{"location":"user-guide/browser-profiles/edit-browser-profile/#description","title":"Description","text":""},{"location":"user-guide/browser-profiles/edit-browser-profile/#tags","title":"Tags","text":""},{"location":"user-guide/browser-profiles/usage-in-crawls/","title":"Usage in Crawls","text":"<p>To use a browser profile, choose the browser profile in the crawl workflow\u2019s Browser Settings. The next workflow run will use the browser profile when crawling.</p> <p>To view a list of crawl workflows that use a specific browser profile, filter the crawl workflows list (Crawling &gt; Workflows) using Browser Profile filter, or go to a browser profile\u2019s detail page to view its Related Workflows section.</p>"},{"location":"user-guide/browser-profiles/usage-in-crawls/#effects-of-crawling","title":"Effects of Crawling","text":"<p>The crawling process may modify the browser profile. If website data\u2014such as cookies or session data\u2014is refreshed during a crawl, Browsertrix will automatically update the browser profile with this data when the crawl successfully finishes. This keeps saved sites in sync with sites at crawl time, preventing the use of expired profile data in subsequent crawls.</p> The following fields in the browser profile\u2019s detail page can indicate whether the profile has been modified by a crawl: <p>Links to the relevant crawl if modified, otherwise displays \u201cNever\u201d.</p> <p>Links to the relevant crawl, otherwise displays the name of the user who last edited or created the browser profile.</p>"},{"location":"user-guide/browser-profiles/usage-in-crawls/#modified-by-crawl","title":"Modified by Crawl","text":""},{"location":"user-guide/browser-profiles/usage-in-crawls/#last-modified-by","title":"Last Modified By","text":""}]}